{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local PySpark on SageMaker Studio\n",
    "\n",
    "This notebook shows how to run local PySpark code within a SageMaker Studio notebook. For this example we use the **Data Science - Python3** image and kernel, but this methodology should work for any kernel within SM Studio, including BYO custom images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2024.7.2   |       h06a4308_0         127 KB\n",
      "    certifi-2024.7.4           |  py310h06a4308_0         158 KB\n",
      "    conda-23.11.0              |  py310h06a4308_0         997 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.3 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2024.2.2~ --> pkgs/main::ca-certificates-2024.7.2-h06a4308_0 \n",
      "  certifi            conda-forge/noarch::certifi-2024.2.2-~ --> pkgs/main/linux-64::certifi-2024.7.4-py310h06a4308_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              conda-forge::conda-23.11.0-py310hff52~ --> pkgs/main::conda-23.11.0-py310h06a4308_0 \n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2024.7.4   |       hbcca054_0         151 KB  conda-forge\n",
      "    certifi-2024.7.4           |     pyhd8ed1ab_0         156 KB  conda-forge\n",
      "    conda-23.11.0              |  py310hff52083_1         956 KB  conda-forge\n",
      "    openjdk-22.0.1             |       hb622114_0       173.2 MB  conda-forge\n",
      "    openssl-3.3.1              |       h4bc722e_2         2.8 MB  conda-forge\n",
      "    xorg-libxt-1.3.0           |       hd590300_1         370 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       177.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            conda-forge/linux-64::openjdk-22.0.1-hb622114_0 \n",
      "  xorg-libxt         conda-forge/linux-64::xorg-libxt-1.3.0-hd590300_1 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2024.7.2-h~ --> conda-forge::ca-certificates-2024.7.4-hbcca054_0 \n",
      "  conda              pkgs/main::conda-23.11.0-py310h06a430~ --> conda-forge::conda-23.11.0-py310hff52083_1 \n",
      "  openssl                                  3.2.1-hd590300_1 --> 3.3.1-h4bc722e_2 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/linux-64::certifi-2024.7.4-~ --> conda-forge/noarch::certifi-2024.7.4-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda update -n base -c defaults conda -q\n",
    "%conda install openjdk -q -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in /opt/conda/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bokeh==2.4.0 in /opt/conda/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /opt/conda/lib/python3.10/site-packages (from bokeh==2.4.0) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.10/site-packages (from bokeh==2.4.0) (1.22.4)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.10/site-packages (from bokeh==2.4.0) (23.2)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from bokeh==2.4.0) (10.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.10/site-packages (from bokeh==2.4.0) (6.0.1)\n",
      "Requirement already satisfied: tornado>=5.1 in /opt/conda/lib/python3.10/site-packages (from bokeh==2.4.0) (6.4)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from bokeh==2.4.0) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=2.9->bokeh==2.4.0) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas==1.5.1 in /opt/conda/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib==3.5.2 in /opt/conda/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.1) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.1) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.1) (1.22.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.5.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.5.2) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.5.2) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.5.2) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.5.2) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.5.2) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.1) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: psycopg2-binary==2.9.9 in /opt/conda/lib/python3.10/site-packages (2.9.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /opt/conda/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: statsmodels==0.13.2 in /opt/conda/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: scipy==1.7.3 in /opt/conda/lib/python3.10/site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.22.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from statsmodels==0.13.2) (1.5.1)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels==0.13.2) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels==0.13.2) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->statsmodels==0.13.2) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->statsmodels==0.13.2) (2024.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels==0.13.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: symbulate==0.5.7 in /opt/conda/lib/python3.10/site-packages (0.5.7)\n",
      "Requirement already satisfied: seaborn==0.11.2 in /opt/conda/lib/python3.10/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from symbulate==0.5.7) (1.22.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from symbulate==0.5.7) (1.7.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from symbulate==0.5.7) (3.5.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.10/site-packages (from seaborn==0.11.2) (1.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->symbulate==0.5.7) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->symbulate==0.5.7) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->symbulate==0.5.7) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->symbulate==0.5.7) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->symbulate==0.5.7) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->symbulate==0.5.7) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->symbulate==0.5.7) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.23->seaborn==0.11.2) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->symbulate==0.5.7) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tenacity==8.0.1 in /opt/conda/lib/python3.10/site-packages (8.0.1)\n",
      "Requirement already satisfied: SQLAlchemy==2.0.23 in /opt/conda/lib/python3.10/site-packages (2.0.23)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy==2.0.23) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy==2.0.23) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost==2.0.2 in /opt/conda/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: pyarrow==14.0.1 in /opt/conda/lib/python3.10/site-packages (14.0.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost==2.0.2) (1.22.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost==2.0.2) (1.7.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: asyncio==3.4.3 in /opt/conda/lib/python3.10/site-packages (3.4.3)\n",
      "Requirement already satisfied: nest-asyncio==1.5.8 in /opt/conda/lib/python3.10/site-packages (1.5.8)\n",
      "Requirement already satisfied: aiohttp==3.9.1 in /opt/conda/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.1) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.1) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.1) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.1) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp==3.9.1) (3.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: boto3==1.21.33 in /opt/conda/lib/python3.10/site-packages (1.21.33)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.10/site-packages (1.24.46)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3==1.21.33) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from boto3==1.21.33) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore) (2.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fsspec==2023.12.1 in /opt/conda/lib/python3.10/site-packages (2023.12.1)\n",
      "Requirement already satisfied: fastparquet==2023.10.1 in /opt/conda/lib/python3.10/site-packages (2023.10.1)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from fastparquet==2023.10.1) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from fastparquet==2023.10.1) (1.22.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.10/site-packages (from fastparquet==2023.10.1) (2.8.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastparquet==2023.10.1) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet==2023.10.1) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet==2023.10.1) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet==2023.10.1) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: watchtower in /opt/conda/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: boto3<2,>=1.9.253 in /opt/conda/lib/python3.10/site-packages (from watchtower) (1.21.33)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.33 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.9.253->watchtower) (1.24.46)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.9.253->watchtower) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.9.253->watchtower) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.25.0,>=1.24.33->boto3<2,>=1.9.253->watchtower) (2.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.25.0,>=1.24.33->boto3<2,>=1.9.253->watchtower) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.33->boto3<2,>=1.9.253->watchtower) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting polars==1.3.0\n",
      "  Using cached polars-1.3.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Using cached polars-1.3.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "Installing collected packages: polars\n",
      "  Attempting uninstall: polars\n",
      "    Found existing installation: polars 1.0.0\n",
      "    Uninstalling polars-1.0.0:\n",
      "      Successfully uninstalled polars-1.0.0\n",
      "Successfully installed polars-1.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.5.0 \n",
    "%pip install bokeh==2.4.0\n",
    "%pip install pandas==1.5.1 matplotlib==3.5.2 \n",
    "%pip install psycopg2-binary==2.9.9 \n",
    "%pip install scikit-learn==1.0.2 statsmodels==0.13.2 scipy==1.7.3 \n",
    "%pip install symbulate==0.5.7 seaborn==0.11.2 \n",
    "%pip install tenacity==8.0.1 SQLAlchemy==2.0.23\n",
    "%pip install xgboost==2.0.2 pyarrow==14.0.1 \n",
    "%pip install asyncio==3.4.3 nest-asyncio==1.5.8 aiohttp==3.9.1 \n",
    "%pip install boto3==1.21.33 botocore\n",
    "%pip install fsspec==2023.12.1 fastparquet==2023.10.1\n",
    "%pip install watchtower\n",
    "%pip install polars==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize S3 Data within local PySpark\n",
    "* By specifying the `hadoop-aws` jar in our Spark config we're able to access S3 datasets using the s3a file prefix. \n",
    "* Since we've already authenticated ourself to SageMaker Studio , we can use our assumed SageMaker ExecutionRole for any S3 reads/writes by setting the credential provider as `ContainerCredentialsProvider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9ee0ff5b-1e5b-41d8-89f9-e7fdae6b40f2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.4.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.23.19 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      ":: resolution report :: resolve 458ms :: artifacts dl 34ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.hadoop#hadoop-aws;3.4.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.23.19 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9ee0ff5b-1e5b-41d8-89f9-e7fdae6b40f2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/12ms)\n",
      "24/08/16 20:03:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from libs.config.config_vars import CONFIG, ENVIRONMENT, DME_PROJECT, S3_PREFIX, ENVIRONMENT_VARS, S3_DATA_PREFIX, \\\n",
    "    S3_BUCKET\n",
    "\n",
    "env = ENVIRONMENT_VARS\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.4.0\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .config('spark.driver.extraJavaOptions',\n",
    "          '-Dio.netty.tryReflectionSetAccessible=true')\n",
    "    .config('spark.executor.extraJavaOptions',\n",
    "          '-Dio.netty.tryReflectionSetAccessible=true')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            SELECT DISTINCT pvs.source_id, pvs.pipeline_runid\n",
      "                FROM(\n",
      "                    SELECT source_id, trait, MAX(pipeline_runid) AS pipeline_runid\n",
      "                    FROM \"managed\".\"rv_ap_all_pvs\"\n",
      "                    WHERE ap_data_sector = 'CORN_NA_SUMMER'\n",
      "                    AND CAST(source_year as integer) IN (2023)\n",
      "                    AND analysis_type in ('GenoPred')\n",
      "                    AND LOWER(loc) = 'all'\n",
      "                    GROUP BY source_id, trait\n",
      "                ) pvs\n",
      "            INNER JOIN(\n",
      "                SELECT decision_group\n",
      "                    FROM \"managed\".\"rv_ap_sector_experiment_config\"\n",
      "                WHERE ap_data_sector_name = 'CORN_NA_SUMMER' \n",
      "                    AND CAST(analysis_year as integer) IN (2023)\n",
      "                    AND adapt_display = 1\n",
      "            ) asec\n",
      "                ON asec.decision_group = pvs.source_id\n",
      "            INNER JOIN(\n",
      "                SELECT trait \n",
      "                    FROM \"managed\".\"rv_ap_sector_trait_config\"\n",
      "                WHERE ap_data_sector_name = 'CORN_NA_SUMMER'\n",
      "                    AND CAST(analysis_year as integer) IN (2023)\n",
      "                    AND LOWER(distribution_type) IN ('normal', 'norm', 'rating', 'zinb', 'text')\n",
      "                    AND LOWER(direction) IN ('positive', 'negative', 'equal', 'not equal', 'contain', 'contains', 'does not contain', 'equals')\n",
      "                    AND LOWER(dme_metric) != 'na'\n",
      "            ) astc\n",
      "            ON pvs.trait = astc.trait\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dme_sagemaker/dme_sagemaker/libs/denodo/denodo_connection.py:65: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, self.__denodo_con)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id call time: 2.6266045570373535\n",
      "('23SUZYYG461', '2023_W_ALL_STG3_105_2_SNYR', '2023_N_ALL_STG2_100_1_SNYR', '2023_DB_ALL_STG4_95_1_SNYR', '23SUWEYG36Z', '23SUNEYG30A', '2023_N_ALL_STG3_95_1_SNYR', '2023_DB_ALL_STG2_90_1_SNYR', '2023_N_ALL_STG3_100_2_SNYR', '2023_W_ALL_STG2_105_3_SNYR', '23SUWEYG35Z', '2023_W_ZY_STG3_115_2_SNYR', '2023_N_ZY_STG3_95_2_SNYR', '2023_DB_ALL_STG1_105_1_SNYR', '23SUDBYG42A', '2023_DB_ALL_STG4_105_1_SNYR', '2023_DB_ALL_STG2_95_2_SNYR', '2023_W_ALL_STG4_115_1_SNYR', '23SUZYYG430', '23SUDBYG41A', '23SUWEYG34Z', '23SUNEYG30B', '2023_W_ALL_STG2_105_1_SNYR', '2023_DB_ALL_STG2_115_2_SNYR', '2023_E_ALL_STG3_115_1_SNYR', '2023_E_ALL_STG2_115_2_SNYR', '2023_DB_ALL_STG2_90_2_SNYR', '2023_DB_ALL_STG2_85_1_SNYR', '2023_W_ALL_STG4_110_1_SNYR', '2023_E_ALL_STG2_120_2_SNYR', '2023_E_ALL_STG3_120_1_SNYR', '2023_E_ALL_STG3_110_2_SNYR', '2023_E_ALL_STG3_105_1_SNYR', '2023_E_ALL_STG2_105_1_SNYR', '2023_DB_ALL_STG2_100_2_SNYR', '23SUZYYG311', '2023_N_ALL_STG4_80_1_SNYR', '2023_N_ALL_STG4.1_85_1_SNYR', '2023_N_ALL_STG4_85_1_SNYR', '2023_E_ZY_STG3_115_2_SNYR', '2023_N_ALL_STG4_90_2_SNYR', '2023_NTL_ALL_STG4.1_105_1_SNYR', '2023_E_ALL_STG2_105_2_SNYR', '2023_W_ALL_STG2_115_2_SNYR', '2023_N_ALL_STG3_90_2_SNYR', '2023_N_ALL_STG2_80_1_SNYR', '2023_DB_ALL_STG3_110_1_SNYR', '2023_N_ALL_STG4_95_2_SNYR', '2023_N_ALL_STG4.1_95_1_SNYR', '23SUZYYG431', '2023_N_ALL_STG4.1_90_1_SNYR', '2023_W_ALL_STG3_100_2_SNYR', '2023_N_ALL_STG3_85_2_SNYR', '2023_NTL_ALL_STG4.1_120_1_SNYR', '2023_W_ALL_STG2_100_3_SNYR', '2023_DB_ALL_STG1_105_2_SNYR', '2023_N_ALL_STG3_90_1_SNYR', '2023_N_ALL_STG4_95_1_SNYR', '2023_W_ALL_STG2_115_1_SNYR', '2023_W_ALL_STG2_110_1_SNYR', '2023_N_ALL_STG4_100_2_SNYR', '2023_W_ALL_STG3_105_1_SNYR', '2023_NTL_ALL_STG4.1_115_1_SNYR', '2023_DB_ALL_STG2_80_2_SNYR', '2023_N_ALL_STG4_80_2_SNYR', '23SUZYYG411', '2023_N_ALL_STG2_95_1_SNYR', '2023_W_ALL_STG2_100_2_SNYR', '2023_DB_ALL_STG4_110_1_SNYR', '2023_NTL_ALL_STG4.1_110_1_SNYR', '2023_DB_ALL_STG4_100_1_SNYR', '2023_W_ALL_STG4_105_2_SNYR', '23SUZYYG452', '2023_W_ALL_STG3_110_2_SNYR', '2023_DB_ALL_STG2_110_2_SNYR', '2023_N_ALL_STG3_100_1_SNYR', '2023_W_ZY_STG3_110_2_SNYR', '23SUWEYG37Z', '2023_W_ALL_STG3_110_1_SNYR', '23SUZYYG440', '2023_N_ZY_STG3_100_2_SNYR', '2023_W_ZY_STG3_100_2_SNYR', '2023_N_ALL_STG2_85_1_SNYR', '2023_W_ALL_STG2_110_2_SNYR', '2023_E_ALL_STG4_115_1_SNYR', '2023_N_ALL_STG3_95_2_SNYR', '2023_N_ALL_STG4_85_2_SNYR', '2023_N_ALL_STG4_90_1_SNYR', '2023_DB_ALL_STG2_110_1_SNYR', '2023_W_ZY_STG3_105_2_SNYR', '23SUZYYG421', '23SUZYYG472', '2023_W_ALL_STG4_105_1_SNYR', '2023_E_ALL_STG2_110_2_SNYR', '2023_N_ZY_STG3_90_2_SNYR', '2023_DB_ALL_STG4_115_1_SNYR', '2023_N_ALL_STG4.1_80_1_SNYR', '2023_W_ALL_STG4_100_1_SNYR', '2023_N_ALL_STG4_100_1_SNYR', '2023_E_ALL_STG3_110_1_SNYR', '2023_E_ALL_STG2_110_1_SNYR', '2023_E_ZY_STG4_110_1_SNYR', '2023_W_ALL_STG3_100_1_SNYR', '2023_E_ALL_STG2_120_1_SNYR', '2023_E_ALL_STG4_120_1_SNYR', '2023_N_ALL_STG3_85_1_SNYR', '2023_DB_ALL_STG2_100_1_SNYR', '2023_E_ZY_STG3_105_2_SNYR', '2023_E_ALL_STG4_110_1_SNYR', '23SUZYYG420', '2023_DB_ALL_STG2_85_2_SNYR', '2023_E_ALL_STG3_115_2_SNYR', '2023_W_ALL_STG2_110_3_SNYR', '2023_W_ALL_STG2_105_2_SNYR', '2023_NTL_ALL_STG4.1_100_1_SNYR', '2023_DB_ALL_STG2_115_1_SNYR', '23SUZYYG470', '2023_W_ALL_STG4_120_1_SNYR', '23SUZYYG450', '2023_W_ALL_STG2_115_3_SNYR', '2023_E_ALL_STG4_105_1_SNYR', '23SUZYYG451', '2023_DB_ALL_STG2_95_1_SNYR', '2023_N_ALL_STG2_90_1_SNYR', '2023_W_ALL_STG3_115_2_SNYR', '2023_W_ALL_STG3_115_1_SNYR', '2023_DB_ALL_STG2_80_1_SNYR', '2023_W_ALL_STG2_100_1_SNYR', '2023_E_ALL_STG3_105_2_SNYR', '2023_E_ZY_STG3_110_2_SNYR', '2023_E_ALL_STG2_115_1_SNYR', '2023_E_ALL_STG3_120_2_SNYR')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from __future__ import annotations\n",
    "from libs.denodo.denodo_connection import DenodoConnection\n",
    "from libs.dme_sql_queries import *\n",
    "from libs.dme_pyspark_sql_queries import *\n",
    "from libs.metric_utils import *\n",
    "from libs.regression_utils import reg_adjust_parallel_rm_pyspark\n",
    "\n",
    "### testing new breakout process\n",
    "\n",
    "ap_data_sector = \"CORN_NA_SUMMER\"\n",
    "analysis_run_group = \"genomic_prediction\"\n",
    "analysis_year = 2023\n",
    "target_pipeline_runid = \"20240723_00_00_00\"\n",
    "force_refresh = \"True\"\n",
    "breakout_level = \"market_segment\"\n",
    "#current_source_ids = \"('224WNMTYG501')\"\n",
    "# current_source_ids = \"('2024_TPP11_PLC4_140_3_SYNR')\"\n",
    "\n",
    "### init- get current source ID's\n",
    "t0 = time.time()\n",
    "current_source_ids = get_source_ids(ap_data_sector, analysis_year, analysis_run_group, target_pipeline_runid, force_refresh)\n",
    "print(\"source_id call time: {0}\".format((time.time() - t0)))\n",
    "print(current_source_ids)\n",
    "\n",
    "# spark.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        SELECT \n",
      "                            ap_data_sector_name,\n",
      "                            spirit_crop_guid\n",
      "                          FROM \"managed\".\"rv_ap_data_sector_config\"\n",
      "                        WHERE \"ap_data_sector_name\" = 'CORN_NA_SUMMER'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dme_sagemaker/dme_sagemaker/libs/denodo/denodo_connection.py:65: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, self.__denodo_con)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT DISTINCT\n",
      "        \"checks\".\"ap_data_sector\" AS \"ap_data_sector\",\n",
      "        \"checks\".\"analysis_year\" AS \"analysis_year\",\n",
      "        \"checks\".\"decision_group\" AS \"decision_group\",\n",
      "        \"checks\".\"untested_entry_display\" AS \"untested_entry_display\",\n",
      "        \"checks\".\"be_bid\" AS \"be_bid\",\n",
      "        CAST(MAX(checks.\"cpifl\",\n",
      "             \"checks\".\"cperf\", \n",
      "             \"checks\".\"cagrf\", \n",
      "             \"checks\".\"cmatf\",\n",
      "             \"checks\".\"cregf\",\n",
      "             \"checks\".\"crtnf\") AS boolean) AS \"cpifl\",\n",
      "        \"checks\".\"cperf\" AS \"cperf\",\n",
      "        \"checks\".\"cagrf\" AS \"cagrf\",\n",
      "        \"checks\".\"cmatf\" AS \"cmatf\",\n",
      "        \"checks\".\"cregf\" AS \"cregf\",\n",
      "        \"checks\".\"crtnf\" AS \"crtnf\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool1' \n",
      "                THEN checks.fp_ltb\n",
      "            WHEN cmt.mp_het_pool = 'pool1'\n",
      "                THEN checks.mp_ltb\n",
      "            WHEN  \"cmt\".\"tester_role\" = 'M' \n",
      "                THEN CAST(MAX(\"checks\".\"cpifl\",\n",
      "                 \"checks\".\"cperf\", \n",
      "                 \"checks\".\"cagrf\", \n",
      "                 \"checks\".\"cmatf\",\n",
      "                 \"checks\".\"cregf\",\n",
      "                 \"checks\".\"crtnf\") AS boolean)\n",
      "             ELSE checks.fp_ltb\n",
      "        END AS \"fp_ltb\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool2' \n",
      "                THEN checks.mp_ltb\n",
      "            WHEN cmt.fp_het_pool = 'pool2'\n",
      "                THEN checks.fp_ltb\n",
      "            WHEN \"cmt\".\"tester_role\" = 'F' \n",
      "                THEN CAST(MAX(\"checks\".\"cpifl\",\n",
      "                 \"checks\".\"cperf\", \n",
      "                 \"checks\".\"cagrf\", \n",
      "                 \"checks\".\"cmatf\",\n",
      "                 \"checks\".\"cregf\",\n",
      "                 \"checks\".\"crtnf\") AS boolean)\n",
      "             ELSE checks.mp_ltb \n",
      "        END AS \"mp_ltb\",\n",
      "        \"checks\".\"count\" AS \"result_count\",\n",
      "        'entry' AS \"material_type\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool1'\n",
      "                THEN \"cmt\".\"fp_be_bid\"\n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool1'\n",
      "                THEN \"cmt\".\"mp_be_bid\"\n",
      "            ELSE COALESCE(\"cmt\".\"fp_be_bid\", \"bbal\".\"receiver_p\")\n",
      "        END AS \"par1_be_bid\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool2'\n",
      "                THEN \"cmt\".\"mp_be_bid\"\n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool2'\n",
      "                THEN \"cmt\".\"fp_be_bid\"\n",
      "            ELSE COALESCE(\"cmt\".\"mp_be_bid\", \"bbal\".\"donor_p\")\n",
      "        END AS \"par2_be_bid\"\n",
      "    FROM(\n",
      "        select \n",
      "\t\t\tec.ap_data_sector_name AS ap_data_sector,\n",
      "\t\t\tec.analysis_year,\n",
      "\t\t\tec.decision_group,\n",
      "\t\t\tmbu.be_bid,\n",
      "\t\t\tmax(ec.untested_entry_display) as untested_entry_display,\n",
      "\t\t    max(CASE WHEN check_agronomic AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cagrf,\n",
      "\t\t    max(CASE WHEN check_competitor AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as ccomp,\n",
      "\t\t    max(CASE WHEN check_flag AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cpifl,\n",
      "\t\t    max(CASE WHEN check_line_to_beat_f AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as fp_ltb,\n",
      "\t\t    max(CASE WHEN check_line_to_beat_m AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as mp_ltb,\n",
      "\t\t    max(CASE WHEN check_maturity AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cmatf,\n",
      "\t\t    max(CASE WHEN check_performance AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cperf,\n",
      "\t\t    max(CASE WHEN check_regional AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cregf,\n",
      "\t\t    max(CASE WHEN check_registration AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as crtnf\n",
      "        from (\n",
      "            select distinct \n",
      "                ex.code          as experiment_id,\n",
      "                m.be_bid \t\t\t\t\tas be_bid,\n",
      "                coalesce(td.check_agronomic, p.check_agronomic, false) as check_agronomic,\n",
      "                coalesce(td.check_competitor, p.check_competitor, false) as check_competitor,\n",
      "                coalesce(td.check_flag, p.check_flag, false) as check_flag,\n",
      "                coalesce(td.check_line_to_beat_f, p.check_line_to_beat_f, false) as check_line_to_beat_f,\n",
      "                coalesce(td.check_line_to_beat_m, p.check_line_to_beat_m, false) as check_line_to_beat_m,\n",
      "                coalesce(td.check_maturity, p.check_maturity, false) as check_maturity,\n",
      "                coalesce(td.check_performance, p.check_performance, false) as check_performance,\n",
      "                coalesce(td.check_regional, p.check_regional, false) as check_regional,\n",
      "                coalesce(td.check_registration, p.check_registration, false) as check_registration\n",
      "            from managed.rv_experiments_loc360 ex\n",
      "            join managed.rv_trials_loc360 t on t.experiment_uuid = ex.object_uuid\n",
      "            join managed.rv_mapped_plot_stats_loc360 p on p.trial_uuid = t.object_uuid\n",
      "            join managed.rv_bb_material_sdl m on lower(m.material_guid) = p.material_batch_uuid\n",
      "            left outer join(\n",
      "\t\t\t    select distinct \n",
      "\t\t\t\t\texperiment_id,\n",
      "\t\t\t\t\tbe_bid,\n",
      "\t\t\t\t\tcoalesce(cpi, 0) = 1 as check_flag,\n",
      "\t\t\t\t\tcoalesce(check_agronomic, 0) = 1 as check_agronomic,\n",
      "\t\t\t\t\tcoalesce(check_maturity, 0) = 1 as check_maturity,\n",
      "\t\t\t\t\tcoalesce(check_performance, 0) = 1 as check_performance,\n",
      "\t\t\t\t\tcoalesce(check_regional, 0) = 1 as check_regional,\n",
      "\t\t\t\t\tcoalesce(check_registration, 0) = 1 as check_registration,\n",
      "\t\t\t\t\tcoalesce(check_line_to_beat_f, 0) = 1 as check_line_to_beat_f,\n",
      "\t\t\t\t\tcoalesce(check_line_to_beat_m, 0) = 1 as check_line_to_beat_m,\n",
      "\t\t\t\t\tcoalesce(check_competitor, 0) = 1 as check_competitor\n",
      "\t\t\t\tfrom managed.rv_bb_experiment_trial_entry_sdl) td\n",
      "\t\t\ton td.experiment_id = ex.code and td.be_bid = m.be_bid\n",
      "\t\t) mbu\n",
      "        inner join (\n",
      "            select ap_data_sector_name,\n",
      "                analysis_year,\n",
      "                max(cast(substring(experiment_id, 0, 2) as integer)) OVER (partition by decision_group) as max_exp_year,\n",
      "                cast(substring(experiment_id, 0, 2) as integer) as exp_year,\n",
      "                experiment_id,\n",
      "                decision_group,\n",
      "                untested_entry_display\n",
      "            from managed.rv_ap_sector_experiment_config\n",
      "            where decision_group is not null\n",
      "            and ap_data_sector_name = 'CORN_NA_SUMMER' \n",
      "            AND analysis_year in 2023\n",
      "        ) ec \n",
      "        on ec.experiment_id = mbu.experiment_id \n",
      "        group by ec.ap_data_sector_name,\n",
      "        ec.analysis_year,\n",
      "        ec.decision_group,\n",
      "        mbu.be_bid\n",
      "    ) \"checks\"\n",
      "    LEFT JOIN \"managed\".\"rv_corn_material_tester_adapt\" \"cmt\"\n",
      "      ON \"checks\".\"be_bid\" = \"cmt\".\"be_bid\"\n",
      "        AND \"checks\".\"ap_data_sector\" LIKE 'CORN%'\n",
      "    LEFT JOIN (\n",
      "        SELECT *\n",
      "          FROM \"managed\".\"rv_be_bid_ancestry_laas\" \n",
      "        WHERE \"receiver_p\" <> \"donor_p\"\n",
      "        AND (\"receiver_p\" IS NOT NULL OR \"donor_p\" IS NOT NULL)\n",
      "    )\"bbal\"\n",
      "    ON \"checks\".\"be_bid\" = \"bbal\".\"be_bid\"\n",
      "        \n",
      "\n",
      "    SELECT DISTINCT\n",
      "        \"checks\".\"ap_data_sector\" AS \"ap_data_sector\",\n",
      "        \"checks\".\"analysis_year\" AS \"analysis_year\",\n",
      "        \"checks\".\"decision_group\" AS \"decision_group\",\n",
      "        \"checks\".\"untested_entry_display\" AS \"untested_entry_display\",\n",
      "        \"checks\".\"be_bid\" AS \"be_bid\",\n",
      "        CAST(MAX(checks.\"cpifl\",\n",
      "             \"checks\".\"cperf\", \n",
      "             \"checks\".\"cagrf\", \n",
      "             \"checks\".\"cmatf\",\n",
      "             \"checks\".\"cregf\",\n",
      "             \"checks\".\"crtnf\") AS boolean) AS \"cpifl\",\n",
      "        \"checks\".\"cperf\" AS \"cperf\",\n",
      "        \"checks\".\"cagrf\" AS \"cagrf\",\n",
      "        \"checks\".\"cmatf\" AS \"cmatf\",\n",
      "        \"checks\".\"cregf\" AS \"cregf\",\n",
      "        \"checks\".\"crtnf\" AS \"crtnf\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool1' \n",
      "                THEN checks.fp_ltb\n",
      "            WHEN cmt.mp_het_pool = 'pool1'\n",
      "                THEN checks.mp_ltb\n",
      "            WHEN  \"cmt\".\"tester_role\" = 'M' \n",
      "                THEN CAST(MAX(\"checks\".\"cpifl\",\n",
      "                 \"checks\".\"cperf\", \n",
      "                 \"checks\".\"cagrf\", \n",
      "                 \"checks\".\"cmatf\",\n",
      "                 \"checks\".\"cregf\",\n",
      "                 \"checks\".\"crtnf\") AS boolean)\n",
      "             ELSE checks.fp_ltb\n",
      "        END AS \"fp_ltb\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool2' \n",
      "                THEN checks.mp_ltb\n",
      "            WHEN cmt.fp_het_pool = 'pool2'\n",
      "                THEN checks.fp_ltb\n",
      "            WHEN \"cmt\".\"tester_role\" = 'F' \n",
      "                THEN CAST(MAX(\"checks\".\"cpifl\",\n",
      "                 \"checks\".\"cperf\", \n",
      "                 \"checks\".\"cagrf\", \n",
      "                 \"checks\".\"cmatf\",\n",
      "                 \"checks\".\"cregf\",\n",
      "                 \"checks\".\"crtnf\") AS boolean)\n",
      "             ELSE checks.mp_ltb \n",
      "        END AS \"mp_ltb\",\n",
      "        \"checks\".\"count\" AS \"result_count\",\n",
      "        'entry' AS \"material_type\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool1'\n",
      "                THEN \"cmt\".\"fp_be_bid\"\n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool1'\n",
      "                THEN \"cmt\".\"mp_be_bid\"\n",
      "            ELSE COALESCE(\"cmt\".\"fp_be_bid\", \"bbal\".\"receiver_p\")\n",
      "        END AS \"par1_be_bid\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool2'\n",
      "                THEN \"cmt\".\"mp_be_bid\"\n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool2'\n",
      "                THEN \"cmt\".\"fp_be_bid\"\n",
      "            ELSE COALESCE(\"cmt\".\"mp_be_bid\", \"bbal\".\"donor_p\")\n",
      "        END AS \"par2_be_bid\"\n",
      "    FROM(\n",
      "        select \n",
      "\t\t\tec.ap_data_sector_name AS ap_data_sector,\n",
      "\t\t\tec.analysis_year,\n",
      "\t\t\tec.decision_group,\n",
      "\t\t\tmbu.be_bid,\n",
      "\t\t\tmax(ec.untested_entry_display) as untested_entry_display,\n",
      "\t\t    max(CASE WHEN check_agronomic AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cagrf,\n",
      "\t\t    max(CASE WHEN check_competitor AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as ccomp,\n",
      "\t\t    max(CASE WHEN check_flag AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cpifl,\n",
      "\t\t    max(CASE WHEN check_line_to_beat_f AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as fp_ltb,\n",
      "\t\t    max(CASE WHEN check_line_to_beat_m AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as mp_ltb,\n",
      "\t\t    max(CASE WHEN check_maturity AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cmatf,\n",
      "\t\t    max(CASE WHEN check_performance AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cperf,\n",
      "\t\t    max(CASE WHEN check_regional AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cregf,\n",
      "\t\t    max(CASE WHEN check_registration AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as crtnf\n",
      "        from (\n",
      "            select distinct \n",
      "                ex.code          as experiment_id,\n",
      "                m.be_bid \t\t\t\t\tas be_bid,\n",
      "                coalesce(td.check_agronomic, p.check_agronomic, false) as check_agronomic,\n",
      "                coalesce(td.check_competitor, p.check_competitor, false) as check_competitor,\n",
      "                coalesce(td.check_flag, p.check_flag, false) as check_flag,\n",
      "                coalesce(td.check_line_to_beat_f, p.check_line_to_beat_f, false) as check_line_to_beat_f,\n",
      "                coalesce(td.check_line_to_beat_m, p.check_line_to_beat_m, false) as check_line_to_beat_m,\n",
      "                coalesce(td.check_maturity, p.check_maturity, false) as check_maturity,\n",
      "                coalesce(td.check_performance, p.check_performance, false) as check_performance,\n",
      "                coalesce(td.check_regional, p.check_regional, false) as check_regional,\n",
      "                coalesce(td.check_registration, p.check_registration, false) as check_registration\n",
      "            from managed.rv_experiments_loc360 ex\n",
      "            join managed.rv_trials_loc360 t on t.experiment_uuid = ex.object_uuid\n",
      "            join managed.rv_mapped_plot_stats_loc360 p on p.trial_uuid = t.object_uuid\n",
      "            join managed.rv_bb_material_sdl m on lower(m.material_guid) = p.material_batch_uuid\n",
      "            left outer join(\n",
      "\t\t\t    select distinct \n",
      "\t\t\t\t\texperiment_id,\n",
      "\t\t\t\t\tbe_bid,\n",
      "\t\t\t\t\tcoalesce(cpi, 0) = 1 as check_flag,\n",
      "\t\t\t\t\tcoalesce(check_agronomic, 0) = 1 as check_agronomic,\n",
      "\t\t\t\t\tcoalesce(check_maturity, 0) = 1 as check_maturity,\n",
      "\t\t\t\t\tcoalesce(check_performance, 0) = 1 as check_performance,\n",
      "\t\t\t\t\tcoalesce(check_regional, 0) = 1 as check_regional,\n",
      "\t\t\t\t\tcoalesce(check_registration, 0) = 1 as check_registration,\n",
      "\t\t\t\t\tcoalesce(check_line_to_beat_f, 0) = 1 as check_line_to_beat_f,\n",
      "\t\t\t\t\tcoalesce(check_line_to_beat_m, 0) = 1 as check_line_to_beat_m,\n",
      "\t\t\t\t\tcoalesce(check_competitor, 0) = 1 as check_competitor\n",
      "\t\t\t\tfrom managed.rv_bb_experiment_trial_entry_sdl) td\n",
      "\t\t\ton td.experiment_id = ex.code and td.be_bid = m.be_bid\n",
      "\t\t) mbu\n",
      "        inner join (\n",
      "            select ap_data_sector_name,\n",
      "                analysis_year,\n",
      "                max(cast(substring(experiment_id, 0, 2) as integer)) OVER (partition by decision_group) as max_exp_year,\n",
      "                cast(substring(experiment_id, 0, 2) as integer) as exp_year,\n",
      "                experiment_id,\n",
      "                decision_group,\n",
      "                untested_entry_display\n",
      "            from managed.rv_ap_sector_experiment_config\n",
      "            where decision_group is not null\n",
      "            and ap_data_sector_name = 'CORN_NA_SUMMER' \n",
      "            AND analysis_year in 2023\n",
      "        ) ec \n",
      "        on ec.experiment_id = mbu.experiment_id \n",
      "        group by ec.ap_data_sector_name,\n",
      "        ec.analysis_year,\n",
      "        ec.decision_group,\n",
      "        mbu.be_bid\n",
      "    ) \"checks\"\n",
      "    LEFT JOIN \"managed\".\"rv_corn_material_tester_adapt\" \"cmt\"\n",
      "      ON \"checks\".\"be_bid\" = \"cmt\".\"be_bid\"\n",
      "        AND \"checks\".\"ap_data_sector\" LIKE 'CORN%'\n",
      "    LEFT JOIN (\n",
      "        SELECT *\n",
      "          FROM \"managed\".\"rv_be_bid_ancestry_laas\" \n",
      "        WHERE \"receiver_p\" <> \"donor_p\"\n",
      "        AND (\"receiver_p\" IS NOT NULL OR \"donor_p\" IS NOT NULL)\n",
      "    )\"bbal\"\n",
      "    ON \"checks\".\"be_bid\" = \"bbal\".\"be_bid\"\n",
      "        \n",
      "\n",
      "    SELECT DISTINCT\n",
      "        \"checks\".\"ap_data_sector\" AS \"ap_data_sector\",\n",
      "        \"checks\".\"analysis_year\" AS \"analysis_year\",\n",
      "        \"checks\".\"decision_group\" AS \"decision_group\",\n",
      "        \"checks\".\"untested_entry_display\" AS \"untested_entry_display\",\n",
      "        \"checks\".\"be_bid\" AS \"be_bid\",\n",
      "        CAST(MAX(checks.\"cpifl\",\n",
      "             \"checks\".\"cperf\", \n",
      "             \"checks\".\"cagrf\", \n",
      "             \"checks\".\"cmatf\",\n",
      "             \"checks\".\"cregf\",\n",
      "             \"checks\".\"crtnf\") AS boolean) AS \"cpifl\",\n",
      "        \"checks\".\"cperf\" AS \"cperf\",\n",
      "        \"checks\".\"cagrf\" AS \"cagrf\",\n",
      "        \"checks\".\"cmatf\" AS \"cmatf\",\n",
      "        \"checks\".\"cregf\" AS \"cregf\",\n",
      "        \"checks\".\"crtnf\" AS \"crtnf\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool1' \n",
      "                THEN checks.fp_ltb\n",
      "            WHEN cmt.mp_het_pool = 'pool1'\n",
      "                THEN checks.mp_ltb\n",
      "            WHEN  \"cmt\".\"tester_role\" = 'M' \n",
      "                THEN CAST(MAX(\"checks\".\"cpifl\",\n",
      "                 \"checks\".\"cperf\", \n",
      "                 \"checks\".\"cagrf\", \n",
      "                 \"checks\".\"cmatf\",\n",
      "                 \"checks\".\"cregf\",\n",
      "                 \"checks\".\"crtnf\") AS boolean)\n",
      "             ELSE checks.fp_ltb\n",
      "        END AS \"fp_ltb\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool2' \n",
      "                THEN checks.mp_ltb\n",
      "            WHEN cmt.fp_het_pool = 'pool2'\n",
      "                THEN checks.fp_ltb\n",
      "            WHEN \"cmt\".\"tester_role\" = 'F' \n",
      "                THEN CAST(MAX(\"checks\".\"cpifl\",\n",
      "                 \"checks\".\"cperf\", \n",
      "                 \"checks\".\"cagrf\", \n",
      "                 \"checks\".\"cmatf\",\n",
      "                 \"checks\".\"cregf\",\n",
      "                 \"checks\".\"crtnf\") AS boolean)\n",
      "             ELSE checks.mp_ltb \n",
      "        END AS \"mp_ltb\",\n",
      "        \"checks\".\"count\" AS \"result_count\",\n",
      "        'entry' AS \"material_type\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool1'\n",
      "                THEN \"cmt\".\"fp_be_bid\"\n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool1'\n",
      "                THEN \"cmt\".\"mp_be_bid\"\n",
      "            ELSE COALESCE(\"cmt\".\"fp_be_bid\", \"bbal\".\"receiver_p\")\n",
      "        END AS \"par1_be_bid\",\n",
      "        CASE \n",
      "            WHEN \"cmt\".\"mp_het_pool\" = 'pool2'\n",
      "                THEN \"cmt\".\"mp_be_bid\"\n",
      "            WHEN \"cmt\".\"fp_het_pool\" = 'pool2'\n",
      "                THEN \"cmt\".\"fp_be_bid\"\n",
      "            ELSE COALESCE(\"cmt\".\"mp_be_bid\", \"bbal\".\"donor_p\")\n",
      "        END AS \"par2_be_bid\"\n",
      "    FROM(\n",
      "        select \n",
      "\t\t\tec.ap_data_sector_name AS ap_data_sector,\n",
      "\t\t\tec.analysis_year,\n",
      "\t\t\tec.decision_group,\n",
      "\t\t\tmbu.be_bid,\n",
      "\t\t\tmax(ec.untested_entry_display) as untested_entry_display,\n",
      "\t\t    max(CASE WHEN check_agronomic AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cagrf,\n",
      "\t\t    max(CASE WHEN check_competitor AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as ccomp,\n",
      "\t\t    max(CASE WHEN check_flag AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cpifl,\n",
      "\t\t    max(CASE WHEN check_line_to_beat_f AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as fp_ltb,\n",
      "\t\t    max(CASE WHEN check_line_to_beat_m AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as mp_ltb,\n",
      "\t\t    max(CASE WHEN check_maturity AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cmatf,\n",
      "\t\t    max(CASE WHEN check_performance AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cperf,\n",
      "\t\t    max(CASE WHEN check_regional AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as cregf,\n",
      "\t\t    max(CASE WHEN check_registration AND ec.exp_year = ec.max_exp_year THEN 1 else 0 end) = 1 as crtnf\n",
      "        from (\n",
      "            select distinct \n",
      "                ex.code          as experiment_id,\n",
      "                m.be_bid \t\t\t\t\tas be_bid,\n",
      "                coalesce(td.check_agronomic, p.check_agronomic, false) as check_agronomic,\n",
      "                coalesce(td.check_competitor, p.check_competitor, false) as check_competitor,\n",
      "                coalesce(td.check_flag, p.check_flag, false) as check_flag,\n",
      "                coalesce(td.check_line_to_beat_f, p.check_line_to_beat_f, false) as check_line_to_beat_f,\n",
      "                coalesce(td.check_line_to_beat_m, p.check_line_to_beat_m, false) as check_line_to_beat_m,\n",
      "                coalesce(td.check_maturity, p.check_maturity, false) as check_maturity,\n",
      "                coalesce(td.check_performance, p.check_performance, false) as check_performance,\n",
      "                coalesce(td.check_regional, p.check_regional, false) as check_regional,\n",
      "                coalesce(td.check_registration, p.check_registration, false) as check_registration\n",
      "            from managed.rv_experiments_loc360 ex\n",
      "            join managed.rv_trials_loc360 t on t.experiment_uuid = ex.object_uuid\n",
      "            join managed.rv_mapped_plot_stats_loc360 p on p.trial_uuid = t.object_uuid\n",
      "            join managed.rv_bb_material_sdl m on lower(m.material_guid) = p.material_batch_uuid\n",
      "            left outer join(\n",
      "\t\t\t    select distinct \n",
      "\t\t\t\t\texperiment_id,\n",
      "\t\t\t\t\tbe_bid,\n",
      "\t\t\t\t\tcoalesce(cpi, 0) = 1 as check_flag,\n",
      "\t\t\t\t\tcoalesce(check_agronomic, 0) = 1 as check_agronomic,\n",
      "\t\t\t\t\tcoalesce(check_maturity, 0) = 1 as check_maturity,\n",
      "\t\t\t\t\tcoalesce(check_performance, 0) = 1 as check_performance,\n",
      "\t\t\t\t\tcoalesce(check_regional, 0) = 1 as check_regional,\n",
      "\t\t\t\t\tcoalesce(check_registration, 0) = 1 as check_registration,\n",
      "\t\t\t\t\tcoalesce(check_line_to_beat_f, 0) = 1 as check_line_to_beat_f,\n",
      "\t\t\t\t\tcoalesce(check_line_to_beat_m, 0) = 1 as check_line_to_beat_m,\n",
      "\t\t\t\t\tcoalesce(check_competitor, 0) = 1 as check_competitor\n",
      "\t\t\t\tfrom managed.rv_bb_experiment_trial_entry_sdl) td\n",
      "\t\t\ton td.experiment_id = ex.code and td.be_bid = m.be_bid\n",
      "\t\t) mbu\n",
      "        inner join (\n",
      "            select ap_data_sector_name,\n",
      "                analysis_year,\n",
      "                max(cast(substring(experiment_id, 0, 2) as integer)) OVER (partition by decision_group) as max_exp_year,\n",
      "                cast(substring(experiment_id, 0, 2) as integer) as exp_year,\n",
      "                experiment_id,\n",
      "                decision_group,\n",
      "                untested_entry_display\n",
      "            from managed.rv_ap_sector_experiment_config\n",
      "            where decision_group is not null\n",
      "            and ap_data_sector_name = 'CORN_NA_SUMMER' \n",
      "            AND analysis_year in 2023\n",
      "        ) ec \n",
      "        on ec.experiment_id = mbu.experiment_id \n",
      "        group by ec.ap_data_sector_name,\n",
      "        ec.analysis_year,\n",
      "        ec.decision_group,\n",
      "        mbu.be_bid\n",
      "    ) \"checks\"\n",
      "    LEFT JOIN \"managed\".\"rv_corn_material_tester_adapt\" \"cmt\"\n",
      "      ON \"checks\".\"be_bid\" = \"cmt\".\"be_bid\"\n",
      "        AND \"checks\".\"ap_data_sector\" LIKE 'CORN%'\n",
      "    LEFT JOIN (\n",
      "        SELECT *\n",
      "          FROM \"managed\".\"rv_be_bid_ancestry_laas\" \n",
      "        WHERE \"receiver_p\" <> \"donor_p\"\n",
      "        AND (\"receiver_p\" IS NOT NULL OR \"donor_p\" IS NOT NULL)\n",
      "    )\"bbal\"\n",
      "    ON \"checks\".\"be_bid\" = \"bbal\".\"be_bid\"\n",
      "        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### get checks\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_sector_config \u001b[38;5;241m=\u001b[39m get_data_sector_config(ap_data_sector)\n\u001b[0;32m----> 4\u001b[0m checks_df \u001b[38;5;241m=\u001b[39m \u001b[43mquery_check_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43map_data_sector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43manalysis_year\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial number of rows in checks df: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(checks_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of unique source id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms in checks df: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(checks_df\u001b[38;5;241m.\u001b[39mget_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecision_group\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/dme_sagemaker/dme_sagemaker/libs/dme_sql_queries.py:580\u001b[0m, in \u001b[0;36mquery_check_entries\u001b[0;34m(ap_data_sector, analysis_year)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DenodoConnection() \u001b[38;5;28;01mas\u001b[39;00m dc:\n\u001b[1;32m    436\u001b[0m     sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124mSELECT DISTINCT\u001b[39m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchecks\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124map_data_sector\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AS \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124map_data_sector\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m         analysis_year,\n\u001b[1;32m    579\u001b[0m     )\n\u001b[0;32m--> 580\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m \u001b[43mdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_df\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py:324\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py:414\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    413\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### get checks\n",
    "data_sector_config = get_data_sector_config(ap_data_sector)\n",
    "\n",
    "checks_df = query_check_entries(ap_data_sector,\n",
    "                analysis_year)\n",
    "print(\"initial number of rows in checks df: {0}\".format(checks_df.shape[0]))\n",
    "print(\"number of unique source id's in checks df: {0}\".format(checks_df.get_column(\"decision_group\").unique().shape[0]))\n",
    "print(\"number of unique entry id's in checks df: {0}\".format(checks_df.get_column(\"be_bid\").unique().shape[0]))\n",
    "print(checks_df.head(5))\n",
    "\n",
    "checks_df = create_check_df(analysis_run_group, checks_df)\n",
    "\n",
    "print(\"number of rows in final checks df: {0}\".format(checks_df.shape[0]))\n",
    "print(\"number of uniques in final checks df: {0}\".format(checks_df.select(\"ap_data_sector\", \"analysis_year\", \"decision_group\", \"be_bid\", \"material_type\").unique().shape[0]))\n",
    "print(\"number of unique source id's in final checks df: {0}\".format(checks_df.get_column(\"decision_group\").unique().shape[0]))\n",
    "print(\"number of unique entry id's in final checks df: {0}\".format(checks_df.get_column(\"be_bid\").unique().shape[0]))\n",
    "\n",
    "# checks_df[\"par1_be_bid\"] = checks_df.par1_be_bid.fillna('')\n",
    "# checks_df[\"par2_be_bid\"] = checks_df.par2_be_bid.fillna('')\n",
    "\n",
    "# for mat_type in checks_df[\"material_type\"].drop_duplicates():\n",
    "#     print(mat_type)\n",
    "#     print(checks_df.loc[checks_df.material_type == mat_type, [\"cpifl\", \"cperf\", \"cagrf\", \"cmatf\"]].describe())\n",
    "    \n",
    "# print(checks_df.loc[(checks_df.decision_group == '24WNMTYG501') & (checks_df.cperf == 1), :].head(5))\n",
    "\n",
    "checks_df.write_csv(\"notebook_output/checks_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        pvs.ap_data_sector,\n",
      "        pvs.analysis_type,\n",
      "        pvs.source_year AS analysis_year,\n",
      "        pvs.source_id AS decision_group,\n",
      "        dg_rm_list.stage,\n",
      "        dg_rm_list.decision_group_rm,\n",
      "        pvs.market_seg,\n",
      "        COALESCE( pvs.loc, 'all') AS pvs_loc,\n",
      "        pvs.trait,\n",
      "        pvs.entry_identifier AS be_bid,\n",
      "        COALESCE(LOWER(pvs.material_type), 'entry') AS material_type,\n",
      "        COALESCE(CAST(pvs.count AS integer), 0) AS count,\n",
      "        CASE WHEN LOWER(pvs.material_type) IN ('entry', 'hybrid')\n",
      "            THEN MAX(0.0, pvs.prediction)\n",
      "            ELSE pvs.prediction\n",
      "        END AS prediction,\n",
      "        CASE \n",
      "            WHEN pvs.stderr IS NULL \n",
      "                AND LOWER(pvs.material_type) = 'entry' \n",
      "                AND LOWER(trait_cfg.distribution_type) LIKE 'norm%'\n",
      "                THEN pvs.prediction*10\n",
      "            WHEN pvs.stderr > 1000 \n",
      "                AND pvs.prediction < 500\n",
      "                THEN MIN(pvs.stderr, pvs.prediction*10) \n",
      "            ELSE pvs.stderr\n",
      "        END AS stderr,\n",
      "        CASE WHEN TRIM(LOWER(trait_cfg.distribution_type)) ='norm'\n",
      "            THEN 'normal'\n",
      "            WHEN TRIM(LOWER(trait_cfg.distribution_type)) = 'rating' AND TRIM(LOWER(trait_cfg.metric_name)) = 'h2h'\n",
      "            THEN 'normal'\n",
      "            ELSE TRIM(LOWER(trait_cfg.distribution_type))\n",
      "        END AS distribution_type,\n",
      "        TRIM(LOWER(trait_cfg.direction)) AS direction,\n",
      "        trait_cfg.yield_trait,\n",
      "        CASE WHEN TRIM(LOWER(trait_cfg.metric_name)) IN ('agronomic', 'disease')\n",
      "            THEN 'risk'\n",
      "            WHEN TRIM(LOWER(trait_cfg.metric_name)) = 'yield' THEN 'performance'\n",
      "            ELSE TRIM(LOWER(trait_cfg.metric_name))\n",
      "        END AS metric_name,\n",
      "        trait_cfg.dme_chkfl,\n",
      "        trait_cfg.dme_reg_x,\n",
      "        trait_cfg.dme_reg_y,\n",
      "        trait_cfg.dme_rm_est,\n",
      "        trait_cfg.dme_weighted_trait\n",
      "      FROM (\n",
      "        SELECT\n",
      "            ap_data_sector,\n",
      "            CAST(source_year AS integer) AS source_year,\n",
      "            analysis_type,\n",
      "            source_id,\n",
      "            FIRST(pipeline_runid) AS pipeline_runid,\n",
      "            REPLACE(TRIM(LOWER(market_seg)), ' ', '_') AS market_seg,\n",
      "            LOWER(loc) AS loc,\n",
      "            trait,\n",
      "            entry_identifier,\n",
      "            material_type,\n",
      "            FIRST(count) AS count,\n",
      "            FIRST(prediction) AS prediction,\n",
      "            FIRST(stderr) AS stderr\n",
      "          FROM managed.rv_ap_all_pvs rv_ap_all_pvs\n",
      "        WHERE ap_data_sector = 'CORN_BRAZIL_SAFRINHA'\n",
      "          AND CAST(source_year AS integer) IN (2024)\n",
      "          AND analysis_type IN ('SingleExp', 'MultiExp')\n",
      "          AND source_id IN ('2024_TPP11_PLC4_140_3_SYNR')\n",
      "          AND loc = 'ALL'\n",
      "          AND prediction IS NOT NULL\n",
      "        GROUP BY\n",
      "            ap_data_sector,\n",
      "            source_year,\n",
      "            analysis_type,\n",
      "            source_id,\n",
      "            REPLACE(TRIM(LOWER(market_seg)), ' ', '_'),\n",
      "            LOWER(loc),\n",
      "            trait,\n",
      "            entry_identifier,\n",
      "            material_type\n",
      "        ORDER BY pipeline_runid DESC\n",
      "    ) pvs\n",
      "    INNER JOIN (\n",
      "        SELECT\n",
      "            ap_data_sector_name,\n",
      "            CAST(analysis_year AS integer) AS analysis_year,\n",
      "            COALESCE(decision_group, experiment_id) AS decision_group,\n",
      "            AVG(COALESCE( decision_group_rm, 0)) AS decision_group_rm,\n",
      "            MAX(stage) AS stage\n",
      "          FROM managed.rv_ap_sector_experiment_config\n",
      "        WHERE ap_data_sector_name = 'CORN_BRAZIL_SAFRINHA'\n",
      "            AND CAST(analysis_year AS integer) IN (2024)\n",
      "            AND decision_group IN ('2024_TPP11_PLC4_140_3_SYNR')\n",
      "            AND adapt_display = 1\n",
      "        GROUP BY\n",
      "            ap_data_sector_name,\n",
      "            analysis_year,\n",
      "            COALESCE(decision_group, experiment_id)\n",
      "    ) dg_rm_list\n",
      "      ON pvs.source_id = dg_rm_list.decision_group\n",
      "      AND pvs.source_year = dg_rm_list.analysis_year\n",
      "    INNER JOIN (\n",
      "        SELECT\n",
      "            ap_data_sector_name,\n",
      "            CAST(analysis_year AS integer) AS analysis_year,\n",
      "            trait,\n",
      "            FIRST(distribution_type) AS distribution_type,\n",
      "            FIRST(direction) AS direction,\n",
      "            FIRST(conv_operator) AS conv_operator,\n",
      "            FIRST(conv_factor) AS conv_factor,\n",
      "            FIRST(yield_trait) AS yield_trait,\n",
      "            FIRST(level) AS level,\n",
      "            FIRST(dme_metric) AS metric_name,\n",
      "            FIRST(dme_chkfl) AS dme_chkfl,\n",
      "            FIRST(dme_reg_x) AS dme_reg_x,\n",
      "            FIRST(dme_reg_y) AS dme_reg_y,\n",
      "            FIRST(dme_rm_est) AS dme_rm_est,\n",
      "            FIRST(dme_weighted_trait) AS dme_weighted_trait\n",
      "          FROM managed.rv_ap_sector_trait_config\n",
      "        WHERE ap_data_sector_name = 'CORN_BRAZIL_SAFRINHA'\n",
      "            AND TRIM(LOWER(direction)) IN ('positive', 'negative', 'equal', 'not equal', 'equals')\n",
      "            AND CAST(analysis_year AS integer) IN (2024)\n",
      "            AND TRIM(LOWER(distribution_type)) != 'text'\n",
      "            AND TRIM(LOWER(level)) = 'plot'\n",
      "            AND (dme_metric <> 'na' OR dme_reg_x = true OR dme_reg_y = true OR dme_rm_est != 0 OR dme_weighted_trait != 0)\n",
      "        GROUP BY\n",
      "            ap_data_sector_name,\n",
      "            analysis_year,\n",
      "            trait\n",
      "        ORDER BY\n",
      "            distribution_type,\n",
      "            direction\n",
      "    ) trait_cfg\n",
      "      ON trait_cfg.ap_data_sector_name = dg_rm_list.ap_data_sector_name\n",
      "        AND trait_cfg.analysis_year = dg_rm_list.analysis_year\n",
      "        AND trait_cfg.trait = pvs.trait\n",
      "    \n",
      "\n",
      "        SELECT\n",
      "            LOWER(feature_name) AS feature_name,\n",
      "            LOWER(REPLACE(REGEXP(TRIM( TRAILING '.' FROM TRIM(value)), '[ :;*+/&\\\\-]+', '_'),',','')) AS value   \n",
      "        FROM managed.rv_feature_export\n",
      "        WHERE LOWER(feature_name) IN ('meso')\n",
      "            AND market_name = 'CORN_BRAZIL_SAFRINHA'\n",
      "        GROUP BY\n",
      "            feature_name,\n",
      "            value\n",
      "        \n",
      "number of rows in pvs: 40265\n",
      "number of unique source id's in pvs: 1\n",
      "number of unique entry id's in pvs: 722\n",
      "shape: (9, 4)\n",
      "\n",
      " statistic   prediction   stderr      count     \n",
      " ---         ---          ---         ---       \n",
      " str         f64          f64         f64       \n",
      "\n",
      " count       40265.0      38232.0     40265.0   \n",
      " null_count  0.0          2033.0      0.0       \n",
      " mean        708.481716   32.861488   6.774072  \n",
      " std         2595.656911  101.667218  17.697525 \n",
      " min         0.0          0.0         1.0       \n",
      " 25%         0.423254     0.167802    2.0       \n",
      " 50%         2.765224     0.568166    3.0       \n",
      " 75%         18.1763      1.089867    7.0       \n",
      " max         13751.7173   698.7022    842.0     \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries missing check info: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvs_input: number of rows: 40265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique source id's in pvs: 1\n",
      "number of unique entry id's in pvs: 722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|breakout_level|breakout_level_value|\n",
      "+--------------+--------------------+\n",
      "|          meso|                sumt|\n",
      "|          meso|                cnms|\n",
      "|            na|                 all|\n",
      "|          meso|                sogo|\n",
      "|          meso|                padf|\n",
      "|          meso|                trmg|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in pvs after merging in checks and reg: 40265\n",
      "number of unique source id's in pvs x check: 1\n",
      "number of unique entry id's in pvs x check: 722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------------------+--------------------+\n",
      "|summary|            count|        prediction|              cpifl|               chkfl|\n",
      "+-------+-----------------+------------------+-------------------+--------------------+\n",
      "|  count|            40265|             40265|              40265|               40265|\n",
      "|   mean|6.774071774493978| 708.4817157792082|0.05051533589966472|0.003849497081832...|\n",
      "| stddev|17.69752545935651|2595.6569111996187|0.21900851117007597| 0.06192554959334477|\n",
      "|    min|                1|               0.0|                  0|                   0|\n",
      "|    25%|                2| 0.423253951134534|                  0|                   0|\n",
      "|    50%|                3|  2.76522379226292|                  0|                   0|\n",
      "|    75%|                7|           18.1702|                  0|                   0|\n",
      "|    max|              842|        13751.7173|                  1|                   1|\n",
      "+-------+-----------------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvs shape after merging in metric_config: 40265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique source id's in pvs x metric: 1\n",
      "number of unique entry id's in pvs x metric: 722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|breakout_level|breakout_level_value|\n",
      "+--------------+--------------------+\n",
      "|          meso|                sumt|\n",
      "|          meso|                cnms|\n",
      "|            na|                 all|\n",
      "|          meso|                sogo|\n",
      "|          meso|                padf|\n",
      "|          meso|                trmg|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-------------------+--------------------+\n",
      "|summary|             count|        prediction|            stddev|              cpifl|               chkfl|\n",
      "+-------+------------------+------------------+------------------+-------------------+--------------------+\n",
      "|  count|             40265|             40265|             38895|              40265|               40265|\n",
      "|   mean| 6.774071774493978| 708.4817157791889|               NaN|0.05051533589966472|0.003849497081832...|\n",
      "| stddev|17.697525459356626|2595.6569111996178|               NaN| 0.2190085111700764| 0.06192554959334482|\n",
      "|    min|                 1|               0.0|            1.0E-4|                  0|                   0|\n",
      "|    25%|                 2| 0.423253951134534|0.3107563312900829|                  0|                   0|\n",
      "|    50%|                 3|  2.76522379226292|1.1879762203007265|                  0|                   0|\n",
      "|    75%|                 7|           18.1702|  4.09253826321734|                  0|                   0|\n",
      "|    max|               842|        13751.7173|               NaN|                  1|                   1|\n",
      "+-------+------------------+------------------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#pvs pipeline\n",
    "analysis_type = get_analysis_types(analysis_run_group)\n",
    "n_partitions = current_source_ids.count(\",\")+1\n",
    "\n",
    "pvs_input_df = query_pvs_input(ap_data_sector, analysis_year, analysis_run_group, current_source_ids, breakout_level)\n",
    "print(\"number of rows in pvs: {0}\".format(pvs_input_df.shape[0]))\n",
    "print(\"number of unique source id's in pvs: {0}\".format(pvs_input_df.get_column(\"decision_group\").unique().shape[0]))\n",
    "print(\"number of unique entry id's in pvs: {0}\".format(pvs_input_df.get_column(\"be_bid\").unique().shape[0]))\n",
    "print(pvs_input_df.select([\"prediction\", \"stderr\", \"count\"]).describe())\n",
    "pvs_input_df.write_csv('notebook_output/pvs_input.csv')\n",
    "\n",
    "metric_config_sp_df = spark.read.csv('dme_core_pipeline/data/metric_config.csv', inferSchema=True, header=True)\n",
    "regression_config_sp_df = spark.read.csv('dme_core_pipeline/data/regression_cfg.csv', inferSchema=True, header=True)\n",
    "regression_config_sp_df = regression_config_sp_df.filter(\"analysis_year = {0} and ap_data_sector = '{1}' and analysis_type IN {2}\".format(analysis_year, ap_data_sector,analysis_type))\n",
    "\n",
    "pvs_input_sp_df = spark.createDataFrame(pvs_input_df.to_pandas()).repartition(n_partitions, 'decision_group', 'trait')\n",
    "checks_sp_df = spark.createDataFrame(checks_df.to_pandas())\n",
    "\n",
    "pvs_input_sp_df.createOrReplaceTempView('pvs_input')\n",
    "checks_sp_df.createOrReplaceTempView('cpifl_table')\n",
    "metric_config_sp_df.createOrReplaceTempView('metric_cfg')\n",
    "\n",
    "missing_checks_df = pvs_input_sp_df.join(checks_sp_df, ['ap_data_sector', 'analysis_year', 'decision_group', 'be_bid'], 'left')\n",
    "print(\"number of entries missing check info: {0}\".format(missing_checks_df.filter('cpifl IS NULL').count()))\n",
    "\n",
    "print(\"pvs_input: number of rows: {0}\".format(pvs_input_sp_df.count()))\n",
    "print(\"number of unique source id's in pvs: {0}\".format(pvs_input_sp_df.select(\"decision_group\").distinct().count()))\n",
    "print(\"number of unique entry id's in pvs: {0}\".format(pvs_input_sp_df.select(\"be_bid\").distinct().count()))\n",
    "pvs_input_sp_df.select(\"breakout_level\", \"breakout_level_value\").distinct().show()\n",
    "\n",
    "# Set recipe variables\n",
    "alpha = 0.3\n",
    "\n",
    "gr_cols = ['ap_data_sector', 'analysis_year', 'analysis_type', 'decision_group', 'material_type',\n",
    "           'breakout_level', 'breakout_level_value', 'x', 'y']\n",
    "\n",
    "gr_cols2 = ['ap_data_sector', 'analysis_year', 'analysis_type', 'decision_group', \n",
    "            'material_type', 'breakout_level', 'breakout_level_value', 'trait']\n",
    "\n",
    "id_cols = ['be_bid', 'count', 'prediction', 'stddev', 'chkfl']\n",
    "\n",
    "pvs_input_sp_df.createOrReplaceTempView('pvs_input')\n",
    "\n",
    "if regression_config_sp_df.count() > 0:\n",
    "    regression_config_sp_df.createOrReplaceTempView('regression_cfg')\n",
    "    regression_input = merge_pvs_regression_input(spark)\n",
    "    regression_input = regression_input.pandas_api()\n",
    "\n",
    "    pvs_regression_output_df = regression_input.groupby(gr_cols).apply(reg_adjust_parallel_rm_pyspark,\n",
    "                                                                       alpha=alpha)\n",
    "    if pvs_regression_output_df.shape[0] > 0:\n",
    "        pvs_regression_output_df = pvs_regression_output_df.loc[\n",
    "            pvs_regression_output_df.adjusted == 'Yes'].to_spark()\n",
    "        pvs_regression_output_df.createOrReplaceTempView('pvs_reg_output')\n",
    "        pvs_metric_raw_df = merge_pvs_cpifl_regression(spark)\n",
    "        spark.catalog.dropTempView('pvs_reg_output')\n",
    "        \n",
    "        pvs_regression_output_df.printSchema()\n",
    "    else:\n",
    "        pvs_metric_raw_df = merge_pvs_cpifl(spark)\n",
    "else:\n",
    "    pvs_metric_raw_df = merge_pvs_cpifl(spark)\n",
    "\n",
    "\n",
    "print(\"number of rows in pvs after merging in checks and reg: {0}\".format(pvs_metric_raw_df.count()))\n",
    "print(\"number of unique source id's in pvs x check: {0}\".format(pvs_metric_raw_df.select(\"decision_group\").distinct().count()))\n",
    "print(\"number of unique entry id's in pvs x check: {0}\".format(pvs_metric_raw_df.select(\"be_bid\").distinct().count()))\n",
    "pvs_metric_raw_df.select(\"count\", \"prediction\", \"stddev\", \"cpifl\", \"chkfl\").summary().show()\n",
    "\n",
    "    \n",
    "# Compute recipe outputs\n",
    "pvs_metric_raw_df.createOrReplaceTempView('pvs_metric_raw')\n",
    "spark.catalog.dropTempView('regression_cfg')\n",
    "spark.catalog.dropTempView('pvs_input')\n",
    "\n",
    "pvs_df = merge_pvs_config(spark, pvs_metric_raw_df, gr_cols2)\n",
    "spark.catalog.dropTempView('pvs_metric_raw')\n",
    "\n",
    "print(\"pvs shape after merging in metric_config: {0}\".format(pvs_df.count()))\n",
    "print(\"number of unique source id's in pvs x metric: {0}\".format(pvs_df.select(\"decision_group\").distinct().count()))\n",
    "print(\"number of unique entry id's in pvs x metric: {0}\".format(pvs_df.select(\"be_bid\").distinct().count()))\n",
    "pvs_df.select(\"breakout_level\", \"breakout_level_value\").distinct().show()\n",
    "pvs_df.select(\"count\", \"prediction\", \"stddev\", \"cpifl\", \"chkfl\").summary().show()\n",
    "pvs_df.repartition(1).write.csv('notebook_output/pvs_intermediate_output.csv', mode='overwrite', header = True)\n",
    "\n",
    "if pvs_df.count() == 0:\n",
    "    pvs_df = create_empty_out()\n",
    "else:\n",
    "    # pvs_df = pvs_df.pandas_api().pandas_on_spark.apply_batch(run_pvs_metrics, id_cols = id_cols, gr_cols2 = gr_cols2)\n",
    "    pvs_df = pvs_df.toPandas()\n",
    "    pvs_df = run_pvs_metrics(pvs_df, id_cols, gr_cols2)\n",
    "    \n",
    "#print(\"pvs shape after running metrics: {0}\".format(pvs_df.shape))\n",
    "#pvs_df.spark.repartition(1).to_csv('notebook_output/pvs_output.csv')\n",
    "pvs_df.to_csv('notebook_output/pvs_output.csv')\n",
    "\n",
    "#pvs_df.info()\n",
    "#missing_pvs_df = pvs_input_sp_df.join(pvs_df.to_spark(), ['ap_data_sector', 'analysis_type', 'analysis_year', 'decision_group', 'be_bid', 'trait'], 'left')\n",
    "# print(\"number of entries missing metrics info: {0}\".format(missing_pvs_df.filter('abs_mean_prob IS NULL').count()))\n",
    "# missing_pvs_df.filter('abs_mean_prob IS NULL').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT DISTINCT\n",
      "        asec.ap_data_sector,\n",
      "        CAST(asec.analysis_year as integer) as analysis_year,\n",
      "        asec.experiment_id,\n",
      "        asec.decision_group,\n",
      "        asec.decision_group_rm,\n",
      "        asec.stage,\n",
      "        astc.trait,\n",
      "        astc.distribution_type,\n",
      "        astc.direction,\n",
      "        astc.conv_operator,\n",
      "        CAST(astc.conv_factor AS float) as conv_factor,\n",
      "        CAST(astc.yield_trait AS integer) AS yield_trait,\n",
      "        astc.level,\n",
      "        astc.metric_name,\n",
      "        astc.dme_chkfl,\n",
      "        astc.dme_reg_x,\n",
      "        astc.dme_reg_y,\n",
      "        astc.dme_rm_est,\n",
      "        astc.dme_weighted_trait\n",
      "      FROM (\n",
      "        SELECT\n",
      "            CAST(analysis_year AS integer) AS analysis_year,\n",
      "            trait,\n",
      "            FIRST(distribution_type) AS distribution_type,\n",
      "            FIRST(direction) AS direction,\n",
      "            FIRST(conv_operator) AS conv_operator,\n",
      "            FIRST(conv_factor) AS conv_factor,\n",
      "            FIRST(yield_trait) AS yield_trait,\n",
      "            FIRST(level) AS level,\n",
      "            FIRST(dme_metric) AS metric_name,\n",
      "            FIRST(dme_chkfl) AS dme_chkfl,\n",
      "            FIRST(dme_reg_x) AS dme_reg_x,\n",
      "            FIRST(dme_reg_y) AS dme_reg_y,\n",
      "            FIRST(dme_rm_est) AS dme_rm_est,\n",
      "            FIRST(dme_weighted_trait) AS dme_weighted_trait\n",
      "        FROM managed.rv_ap_sector_trait_config\n",
      "        WHERE ap_data_sector_name = 'CORN_BRAZIL_SAFRINHA'\n",
      "            AND CAST(analysis_year AS integer) IN (2024)\n",
      "            AND TRIM(LOWER(distribution_type)) IN ('normal', 'norm', 'rating', 'zinb', 'text')\n",
      "            AND TRIM(LOWER(direction)) IN ('positive', 'negative', 'equal', 'not equal', 'contain', 'contains', 'does not contain', 'equals')\n",
      "            AND LOWER(level) = 'plot'\n",
      "            AND (dme_metric != 'na' OR dme_reg_x = true OR dme_reg_y = true OR dme_rm_est != 0 OR dme_weighted_trait != 0)\n",
      "        GROUP BY\n",
      "            analysis_year,\n",
      "            trait\n",
      "        ORDER BY\n",
      "            distribution_type,\n",
      "            direction\n",
      "    ) astc\n",
      "    INNER JOIN(\n",
      "        SELECT \n",
      "            ap_data_sector_name AS ap_data_sector, \n",
      "            CAST(analysis_year AS integer) AS analysis_year, \n",
      "            experiment_id, \n",
      "            decision_group,\n",
      "            AVG(COALESCE( decision_group_rm, 0)) OVER (PARTITION BY decision_group) AS decision_group_rm,\n",
      "            MAX(stage) OVER (PARTITION BY decision_group) AS stage\n",
      "        FROM managed.rv_ap_sector_experiment_config\n",
      "        WHERE ap_data_sector_name = 'CORN_BRAZIL_SAFRINHA'\n",
      "            AND CAST(analysis_year as integer) IN (2024)\n",
      "            AND decision_group IN ('2024_TPP11_PLC4_140_3_SYNR')\n",
      "    ) asec\n",
      "    ON asec.analysis_year = astc.analysis_year\n",
      "    \n",
      "\n",
      "     SELECT DISTINCT ap_data_sector, CAST(source_year AS integer) AS analysis_year, analysis_type, source_id as decision_group, trait\n",
      "        FROM managed.rv_ap_all_pvs\n",
      "    WHERE ap_data_sector = 'CORN_BRAZIL_SAFRINHA'\n",
      "        AND CAST(source_year as integer) IN (2024)\n",
      "        AND analysis_type IN ('SingleExp', 'MultiExp')\n",
      "        AND LOWER(loc) = 'all'\n",
      "        AND source_id IN ('2024_TPP11_PLC4_140_3_SYNR')\n",
      "    \n",
      "\n",
      "        SELECT\n",
      "            tet.trial_id,\n",
      "            tet.loc_selector,\n",
      "            tet.be_bid,\n",
      "            tet.decision_group_rm,\n",
      "            tet.stage,\n",
      "            tet.experiment_id,\n",
      "            tet.plot_barcode,\n",
      "            tet.trait,\n",
      "            tet.result_numeric_value\n",
      "        FROM (\n",
      "            SELECT\n",
      "                tpad.experiment_id,\n",
      "                tpad.trial_id,\n",
      "                tpad.be_bid,\n",
      "                COALESCE(tpad.maturity_group, 0) AS decision_group_rm,\n",
      "                COALESCE(tpad.trial_stage, 0) AS stage,\n",
      "                tpad.plot_barcode AS plot_barcode,\n",
      "                tpad.loc_selector,\n",
      "                tpad.trait_measure_code AS trait,\n",
      "                tpad.result_numeric_value,\n",
      "                yhat.outlier_flag AS outlier_flag\n",
      "            FROM (\n",
      "                SELECT DISTINCT\n",
      "                    experiment_id,\n",
      "                    trial_id,\n",
      "                    loc_selector,\n",
      "                    plot_barcode,\n",
      "                    be_bid,\n",
      "                    maturity_group,\n",
      "                    trial_stage,\n",
      "                    trait_measure_code,\n",
      "                    result_numeric_value\n",
      "                FROM managed.rv_trial_pheno_analytic_dataset\n",
      "                WHERE NOT CAST(tr_exclude AS boolean)\n",
      "                    AND NOT CAST(psp_exclude AS boolean)\n",
      "                    AND NOT CAST(pr_exclude AS boolean)\n",
      "                    AND result_numeric_value IS NOT NULL\n",
      "                    AND ap_data_sector LIKE 'CORN%'\n",
      "                    AND experiment_id IN ('24WNUBYG405', '24WNUBYG404', '24WNUBYG452', '24WNUBYG453', '24WNUBYG409', '24WNUBYG471', '24WNUBYG407', '24WNUBYG410', '24WNUBYG473', '24WNUBYG472', '24WNUBYG451', '24WNUBYG408', '24WNUBYG403', '24WNUBYG401', '24WNUBYG402')\n",
      "                    AND trait_measure_code IN ('GMSTP', 'YGSMN')\n",
      "            ) tpad\n",
      "            LEFT JOIN(\n",
      "                SELECT \n",
      "                    plot_barcode,\n",
      "                    trait,\n",
      "                    MAX(CASE WHEN outlier_flag=TRUE THEN 1 ELSE 0 END) AS outlier_flag\n",
      "                  FROM managed.rv_ap_all_yhat\n",
      "                  WHERE ap_data_sector = 'CORN_BRAZIL_SAFRINHA'\n",
      "                GROUP BY\n",
      "                    plot_barcode,\n",
      "                    trait\n",
      "            ) yhat\n",
      "            ON yhat.plot_barcode = tpad.plot_barcode\n",
      "                AND yhat.trait = tpad.trait_measure_code\n",
      "        ) tet\n",
      "        WHERE (tet.outlier_flag = 0 or tet.outlier_flag IS NULL)\n",
      "    \n",
      "\n",
      "        SELECT    \n",
      "            source_id,\n",
      "            LOWER(feature_name) AS breakout_level,\n",
      "            LOWER(REPLACE(REGEXP(TRIM( TRAILING '.' FROM TRIM(value)), '[ :;*+/&\\\\-]+', '_'),',','')) AS breakout_level_value   \n",
      "        FROM managed.rv_feature_export\n",
      "        WHERE LOWER(feature_name) IN ('meso')\n",
      "            AND market_name LIKE 'CORN%'\n",
      "        \n",
      "trial data call time: 7.981799602508545\n",
      "compute_trial_comparison_metric_input: trial_numeric_input_df data count=74252\n",
      "compute_trial_comparison_metric_input: trial_numeric_input_df data unique source_id's: 15\n",
      "compute_trial_comparison_metric_input: trial_numeric_input_df data unique entry_id's: 722\n",
      "shape: (5, 26)\n",
      "\n",
      " trial_id    be_bid      decision_g  stage    dme_rm_es  dme_weigh  breakout_  breakout_ \n",
      " ---         ---         roup_rm     ---       t          ted_trait  level      level_val \n",
      " str         str         ---         f64       ---        ---        ---        ue        \n",
      "                         f32                   str        str        str        ---       \n",
      "                                                                                str       \n",
      "\n",
      " 24WNUBYG40  EMF2301896  0.0         4.0      0          0          meso       sogo      \n",
      " 7BU11       638                                                                          \n",
      " 24WNUBYG40  EMF2201023  0.0         4.0      0          0          meso       sogo      \n",
      " 4BU01       168                                                                          \n",
      " 24WNUBYG40  EMF1900500  0.0         4.0      0          0          meso       padf      \n",
      " 4BU24       657                                                                          \n",
      " 24WNUBYG40  EMF1900500  0.0         4.0      0          0          meso       trmg      \n",
      " 7BU39       657                                                                          \n",
      " 24WNUBYG40  EMF2301896  0.0         4.0      0          0          meso       sumt      \n",
      " 7BL28       744                                                                          \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(trial_numeric_input_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     42\u001b[0m n_partitions \u001b[38;5;241m=\u001b[39m (current_source_ids\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 44\u001b[0m trial_data_sp_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame(trial_numeric_input_df\u001b[38;5;241m.\u001b[39mto_pandas())\u001b[38;5;241m.\u001b[39mrepartition(n_partitions, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_group\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrait\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreakout_level\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreakout_level_value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m## Bypass regression\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute_trial_comparison_metric_input: trial_data_sp_df data count=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trial_data_sp_df\u001b[38;5;241m.\u001b[39mcount()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "## stability (trial data) pipeline\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "alpha = 0.3\n",
    "\n",
    "gr_cols = ['ap_data_sector', 'analysis_year', 'trial_id', 'x', 'y']\n",
    "cols = ['ap_data_sector', 'analysis_year', 'trial_id', 'be_bid', 'year', 'experiment_id', 'x', 'y',\n",
    "        'function', 'plot_barcode',\n",
    "        'trait', 'prediction_x', 'prediction', 'analysis_target_y', 'trial_pts', 'analysis_pts',\n",
    "        'adjusted_prediction', 'adj_model',\n",
    "        'adj_outlier', 'p_value', 'slope1', 'slope2', 'intercept', 'residual', 'adjusted']\n",
    "\n",
    "col_partitions = ['ap_data_sector', 'analysis_year', 'decision_group', 'breakout_level',\n",
    "                  'breakout_level_value']\n",
    "\n",
    "metric_input_cols = ['ap_data_sector', 'analysis_year', 'analysis_type', 'decision_group', 'be_bid',\n",
    "                     'material_type', 'breakout_level', 'breakout_level_value',\n",
    "                     'trial_id', 'trait', 'result_numeric_value', 'metric_name',\n",
    "                     'cpifl', 'chkfl', 'distribution_type', 'direction', 'threshold_factor',\n",
    "                     'spread_factor', 'weight', 'adv_weight']\n",
    "\n",
    "gr_cols2 = ['ap_data_sector', 'analysis_year', 'analysis_type', 'decision_group', 'be_bid',\n",
    "            'material_type', 'breakout_level', 'breakout_level_value',\n",
    "            'trait', 'metric_name', 'distribution_type', 'direction']\n",
    "\n",
    "t0 = time.time()\n",
    "trial_numeric_input_df = query_trial_input(ap_data_sector,\n",
    "                            analysis_year,\n",
    "                            analysis_run_group,\n",
    "                            current_source_ids,\n",
    "                            breakout_level,\n",
    "                            'numeric')\n",
    "\n",
    "trial_numeric_input_df.write_csv(\"notebook_output/trial_numeric.csv\")\n",
    "print(\"trial data call time: {0}\".format((time.time() - t0)))\n",
    "\n",
    "print('compute_trial_comparison_metric_input: trial_numeric_input_df data count={0}'.format(trial_numeric_input_df.shape[0]))\n",
    "print(\"compute_trial_comparison_metric_input: trial_numeric_input_df data unique source_id's: {0}\".format(trial_numeric_input_df.get_column(\"experiment_id\").unique().shape[0]))\n",
    "print(\"compute_trial_comparison_metric_input: trial_numeric_input_df data unique entry_id's: {0}\".format(trial_numeric_input_df.get_column(\"be_bid\").unique().shape[0]))\n",
    "print(trial_numeric_input_df.head())\n",
    "\n",
    "n_partitions = (current_source_ids.count(\",\")+1)*10\n",
    "\n",
    "trial_data_sp_df = spark.createDataFrame(trial_numeric_input_df.to_pandas()).repartition(n_partitions, 'decision_group', 'trait', 'breakout_level', 'breakout_level_value')\n",
    "\n",
    "## Bypass regression\n",
    "\n",
    "print('compute_trial_comparison_metric_input: trial_data_sp_df data count={0}'.format(trial_data_sp_df.count()))\n",
    "trial_data_sp_df.show(n=5)\n",
    "\n",
    "trial_data_sp_df.createOrReplaceTempView('tr_data1')\n",
    "\n",
    "# Use cpifl table to get parentage, and then create trial pheno data for parents and append to entry-level data\n",
    "trial_check_sp_df = merge_trial_cpifl(spark, 'numeric')\n",
    "print('compute_trial_comparison_metric_input: trial_check_sp_df count={0}'.format(trial_check_sp_df.count()))\n",
    "\n",
    "trial_window = Window.partitionBy('trial_id')\n",
    "trial_check_sp_df = trial_check_sp_df.withColumn('mincpi',\n",
    "                                                 F.min('cpifl').over(trial_window)) \\\n",
    "    .where(0 == F.col('mincpi')) \\\n",
    "    .drop('mincpi')\n",
    "\n",
    "trial_check_sp_df.repartition(n_partitions, col_partitions).createOrReplaceTempView(\n",
    "    'trc_data3')\n",
    "\n",
    "print('compute_trial_comparison_metric_input_df: After partitionBy trial_check_sp_df count={0}'.format(trial_check_sp_df.count()))\n",
    "\n",
    "\n",
    "# Apply metric config\n",
    "trial_check_met_sp_df = merge_trial_config(spark, 'numeric')\n",
    "print('compute_trial_comparison_metric_input_df: After merge_trial_pheno_config count={0}'.format(trial_check_met_sp_df.count()))\n",
    "\n",
    "trial_check_met_sp_df.createOrReplaceTempView('trial_pheno_metric_input')\n",
    "\n",
    "trial_check_met_sp_df = trial_check_met_sp_df.filter(\n",
    "    \"distribution_type != 'rating'\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Create metric_type = threshold/rating input\n",
    "# Rating metric input\n",
    "trial_rating_metric_input_df2 = prepare_rating_metric_input(trial_check_met_sp_df,\n",
    "                                                            metric_input_cols, gr_cols2)\n",
    "print('compute_trial_comparison_metric_input_df: After prepare_rating_metric_input count={0}'.format(trial_rating_metric_input_df2.count()))\n",
    "# Create metric_type = pct_check output\n",
    "# create h2h structure\n",
    "h2h_input = merge_trial_h2h(spark, trial_check_met_sp_df)\n",
    "print('compute_trial_comparison_metric_input_df: After merge_h2h_placement count={0}'.format(h2h_input.count()))\n",
    "\n",
    "h2h_input = h2h_input.unionByName(trial_rating_metric_input_df2, allowMissingColumns=True)\n",
    "print('compute_trial_comparison_metric_input_df: After unionByName, count={0}'.format(h2h_input.count()))\n",
    "h2h_input.show(n=5)\n",
    "\n",
    "# spark.catalog.dropTempView('trial_pheno_metric_input')\n",
    "# trial_check_met_sp_df.unpersist()\n",
    "# h2h_output = h2h_input.pandas_api().pandas_on_spark.apply_batch(run_trial_metrics)\n",
    "# print('compute_trial_comparison_metric_input_df: Apply run_metrics count={0}'.format(h2h_output.count()))\n",
    "\n",
    "# h2h_output.spark.repartition(1).to_csv('notebook_output/trial_output.csv')\n",
    "\n",
    "h2h_input = h2h_input.toPandas()\n",
    "h2h_output = run_trial_metrics(h2h_input)\n",
    "h2h_output.to_csv('notebook_output/trial_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT DISTINCT\n",
      "        asec.ap_data_sector,\n",
      "        CAST(asec.analysis_year as integer) as analysis_year,\n",
      "        asec.experiment_id,\n",
      "        asec.decision_group,\n",
      "        asec.decision_group_rm,\n",
      "        asec.stage,\n",
      "        astc.trait,\n",
      "        astc.distribution_type,\n",
      "        astc.direction,\n",
      "        astc.conv_operator,\n",
      "        CAST(astc.conv_factor AS float) as conv_factor,\n",
      "        CAST(astc.yield_trait AS integer) AS yield_trait,\n",
      "        astc.level,\n",
      "        astc.metric_name,\n",
      "        astc.dme_chkfl,\n",
      "        astc.dme_reg_x,\n",
      "        astc.dme_reg_y,\n",
      "        astc.dme_rm_est,\n",
      "        astc.dme_weighted_trait\n",
      "      FROM (\n",
      "        SELECT\n",
      "            CAST(analysis_year AS integer) AS analysis_year,\n",
      "            trait,\n",
      "            FIRST(distribution_type) AS distribution_type,\n",
      "            FIRST(direction) AS direction,\n",
      "            FIRST(conv_operator) AS conv_operator,\n",
      "            FIRST(conv_factor) AS conv_factor,\n",
      "            FIRST(yield_trait) AS yield_trait,\n",
      "            FIRST(level) AS level,\n",
      "            FIRST(dme_metric) AS metric_name,\n",
      "            FIRST(dme_chkfl) AS dme_chkfl,\n",
      "            FIRST(dme_reg_x) AS dme_reg_x,\n",
      "            FIRST(dme_reg_y) AS dme_reg_y,\n",
      "            FIRST(dme_rm_est) AS dme_rm_est,\n",
      "            FIRST(dme_weighted_trait) AS dme_weighted_trait\n",
      "        FROM managed.rv_ap_sector_trait_config\n",
      "        WHERE ap_data_sector_name = 'CORN_BRAZIL_SAFRINHA'\n",
      "            AND CAST(analysis_year AS integer) IN (2024)\n",
      "            AND TRIM(LOWER(distribution_type)) IN ('normal', 'norm', 'rating', 'zinb', 'text')\n",
      "            AND TRIM(LOWER(direction)) IN ('positive', 'negative', 'equal', 'not equal', 'contain', 'contains', 'does not contain', 'equals')\n",
      "            AND LOWER(level) = 'plot'\n",
      "            AND (dme_metric != 'na' OR dme_reg_x = true OR dme_reg_y = true OR dme_rm_est != 0 OR dme_weighted_trait != 0)\n",
      "        GROUP BY\n",
      "            analysis_year,\n",
      "            trait\n",
      "        ORDER BY\n",
      "            distribution_type,\n",
      "            direction\n",
      "    ) astc\n",
      "    INNER JOIN(\n",
      "        SELECT \n",
      "            ap_data_sector_name AS ap_data_sector, \n",
      "            CAST(analysis_year AS integer) AS analysis_year, \n",
      "            experiment_id, \n",
      "            decision_group,\n",
      "            AVG(COALESCE( decision_group_rm, 0)) OVER (PARTITION BY decision_group) AS decision_group_rm,\n",
      "            MAX(stage) OVER (PARTITION BY decision_group) AS stage\n",
      "        FROM managed.rv_ap_sector_experiment_config\n",
      "        WHERE ap_data_sector_name = 'CORN_BRAZIL_SAFRINHA'\n",
      "            AND CAST(analysis_year as integer) IN (2024)\n",
      "            AND decision_group IN ('2024_TPP11_PLC4_140_3_SYNR')\n",
      "    ) asec\n",
      "    ON asec.analysis_year = astc.analysis_year\n",
      "    \n",
      "\n",
      "     SELECT DISTINCT ap_data_sector, CAST(source_year AS integer) AS analysis_year, analysis_type, source_id as decision_group, trait\n",
      "        FROM managed.rv_ap_all_pvs\n",
      "    WHERE ap_data_sector = 'CORN_BRAZIL_SAFRINHA'\n",
      "        AND CAST(source_year as integer) IN (2024)\n",
      "        AND analysis_type IN ('SingleExp', 'MultiExp')\n",
      "        AND LOWER(loc) = 'all'\n",
      "        AND source_id IN ('2024_TPP11_PLC4_140_3_SYNR')\n",
      "    \n",
      "\n",
      "        SELECT\n",
      "            tet.trial_id,\n",
      "            tet.loc_selector,\n",
      "            tet.be_bid,\n",
      "            tet.decision_group_rm,\n",
      "            tet.stage,\n",
      "            tet.experiment_id,\n",
      "            tet.plot_barcode,\n",
      "            tet.trait,\n",
      "            tet.result_alpha_value\n",
      "        FROM (\n",
      "            SELECT\n",
      "                tpad.experiment_id,\n",
      "                tpad.trial_id,\n",
      "                tpad.be_bid,\n",
      "                COALESCE(tpad.maturity_group, 0) AS decision_group_rm,\n",
      "                COALESCE(tpad.trial_stage, 0) AS stage,\n",
      "                tpad.plot_barcode AS plot_barcode,\n",
      "                tpad.loc_selector,\n",
      "                tpad.trait_measure_code AS trait,\n",
      "                tpad.result_alpha_value,\n",
      "                0 AS outlier_flag\n",
      "            FROM (\n",
      "                SELECT DISTINCT\n",
      "                    experiment_id,\n",
      "                    trial_id,\n",
      "                    loc_selector,\n",
      "                    plot_barcode,\n",
      "                    be_bid,\n",
      "                    maturity_group,\n",
      "                    trial_stage,\n",
      "                    trait_measure_code,\n",
      "                    result_alpha_value\n",
      "                FROM managed.rv_trial_pheno_analytic_dataset\n",
      "                WHERE NOT CAST(tr_exclude AS boolean)\n",
      "                    AND NOT CAST(psp_exclude AS boolean)\n",
      "                    AND NOT CAST(pr_exclude AS boolean)\n",
      "                    AND result_alpha_value IS NOT NULL\n",
      "                    AND ap_data_sector LIKE 'CORN%'\n",
      "                    AND experiment_id IN ('24WNUBYG401', '24WNUBYG453', '24WNUBYG404', '24WNUBYG410', '24WNUBYG473', '24WNUBYG403', '24WNUBYG472', '24WNUBYG407', '24WNUBYG408', '24WNUBYG409', '24WNUBYG402', '24WNUBYG471', '24WNUBYG405', '24WNUBYG451', '24WNUBYG452')\n",
      "                    AND trait_measure_code IN ('')\n",
      "            ) tpad\n",
      "            \n",
      "        ) tet\n",
      "        WHERE (tet.outlier_flag = 0 or tet.outlier_flag IS NULL)\n",
      "    \n",
      "\n",
      "        SELECT    \n",
      "            source_id,\n",
      "            LOWER(feature_name) AS breakout_level,\n",
      "            LOWER(REPLACE(REGEXP(TRIM( TRAILING '.' FROM TRIM(value)), '[ :;*+/&\\\\-]+', '_'),',','')) AS breakout_level_value   \n",
      "        FROM managed.rv_feature_export\n",
      "        WHERE LOWER(feature_name) IN ('meso')\n",
      "            AND market_name LIKE 'CORN%'\n",
      "        \n",
      "trial alpha data call time: 7.8354408740997314\n"
     ]
    }
   ],
   "source": [
    "## text output\n",
    "\n",
    "t0 = time.time()\n",
    "trial_alpha_input_df = query_trial_input(ap_data_sector,\n",
    "                            analysis_year,\n",
    "                            analysis_run_group,\n",
    "                            current_source_ids,\n",
    "                            breakout_level,\n",
    "                            'alpha')\n",
    "print(\"trial alpha data call time: {0}\".format((time.time() - t0)))\n",
    "\n",
    "### create empty text metric output\n",
    "#text_df = merge_trial_text_input(ap_data_sector, analysis_year, current_source_ids)\n",
    "#text_sp_df = spark.createDataFrame(text_df)\n",
    "text_metric_output_df = create_empty_out(spark).toPandas()\n",
    "\n",
    "# Merges pvs_output and trial_output and then appends text output.\n",
    "\n",
    "# Generate H2H metrics\n",
    "def compute_mti(df, group_cols, weight_col=\"weight\", h2h_mode=False):\n",
    "    base_group_cols = [\n",
    "        \"ap_data_sector\",\n",
    "        \"analysis_year\",\n",
    "        \"analysis_type\",\n",
    "        \"decision_group\",\n",
    "        \"material_type\",\n",
    "        \"breakout_level\",\n",
    "        \"breakout_level_value\",\n",
    "        \"be_bid\",\n",
    "    ]\n",
    "\n",
    "    sum_cols = [\"count\"]\n",
    "    mean_cols = [\"prediction\", \"stddev\"]\n",
    "    max_cols = [\"cpifl\", \"chkfl\"]\n",
    "\n",
    "    if h2h_mode:\n",
    "        sum_cols = sum_cols + [\"check_count\"]\n",
    "        mean_cols = mean_cols + [\"check_prediction\", \"check_stddev\"]\n",
    "        max_cols = max_cols + [\"check_chkfl\"]\n",
    "\n",
    "    if weight_col == None:\n",
    "        mean_cols = mean_cols + [\"weight\", \"adv_weight\", \"pctchk\", \"statistic\"]\n",
    "    elif weight_col == \"weight\":\n",
    "        mean_cols = mean_cols + [\"adv_weight\"]\n",
    "\n",
    "    if weight_col == None:\n",
    "        df = df.group_by(base_group_cols + group_cols).agg(\n",
    "            [pl.col(f\"{c}\").sum() for c in sum_cols]\n",
    "            + [pl.col(f\"{c}\").mean() for c in mean_cols]\n",
    "            + [pl.col(f\"{c}\").max() for c in max_cols]\n",
    "            + [np.exp(np.log(pl.col(\"metric_value\")).mean())]\n",
    "        )\n",
    "    else:\n",
    "        df = df.group_by(base_group_cols + group_cols).agg(\n",
    "            [pl.col(f\"{c}\").sum() for c in sum_cols]\n",
    "            + [pl.col(f\"{c}\").mean() for c in mean_cols]\n",
    "            + [pl.col(f\"{c}\").max() for c in max_cols]\n",
    "            + [\n",
    "                pl.lit(\"aggregate\").alias(\"metric_method\"),\n",
    "                ((pl.col(\"pctchk\") * pl.col(weight_col)).sum())\n",
    "                / (pl.col(weight_col).sum()).alias(\"pctchk\"),\n",
    "                ((pl.col(\"statistic\") * pl.col(weight_col)).sum())\n",
    "                / (pl.col(weight_col).sum()).alias(\"statistic\"),\n",
    "                (\n",
    "                    np.exp(\n",
    "                        ((np.log(pl.col(\"metric_value\")) * pl.col(weight_col)).sum())\n",
    "                        / (pl.col(weight_col).sum())\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if h2h_mode:\n",
    "        df = df.with_columns(pl.lit(\"aggregate\").alias(\"trait\"))\n",
    "\n",
    "    if weight_col == \"adv_weight\":\n",
    "        df = df.with_columns(pl.lit(\"advancement\").alias(\"metric_name\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "base_group_cols = [\n",
    "    \"ap_data_sector\",\n",
    "    \"analysis_year\",\n",
    "    \"analysis_type\",\n",
    "    \"decision_group\",\n",
    "    \"material_type\",\n",
    "    \"breakout_level\",\n",
    "    \"breakout_level_value\",\n",
    "    \"be_bid\",\n",
    "]\n",
    "\n",
    "bebid_data_cols = [\n",
    "    \"count\",\n",
    "    \"prediction\",\n",
    "    \"stddev\",\n",
    "    \"cpifl\",\n",
    "    \"chkfl\",\n",
    "]\n",
    "\n",
    "check_bebid_data_cols = [\n",
    "    \"check_be_bid\",\n",
    "    \"check_count\",\n",
    "    \"check_prediction\",\n",
    "    \"check_stddev\",\n",
    "    \"check_chkfl\",\n",
    "]\n",
    "\n",
    "base_metric_cols = [\n",
    "    \"metric_name\",\n",
    "    \"pctchk\",\n",
    "    \"statistic\",\n",
    "    \"metric_value\",\n",
    "    \"metric_method\",\n",
    "]\n",
    "\n",
    "stacked_df_cols = (\n",
    "    base_group_cols\n",
    "    + [\"trait\", \"weight\", \"adv_weight\"]\n",
    "    + bebid_data_cols\n",
    "    + check_bebid_data_cols\n",
    "    + base_metric_cols\n",
    ")\n",
    "\n",
    "stacked_df = pl.concat(\n",
    "    [\n",
    "        pl.from_pandas(pvs_df).select(stacked_df_cols),\n",
    "        pl.from_pandas(h2h_output).select(stacked_df_cols),\n",
    "        pl.from_pandas(text_metric_output_df).select(stacked_df_cols),\n",
    "    ],\n",
    "    how=\"vertical_relaxed\",\n",
    ")\n",
    "\n",
    "h2h_metric_df = compute_mti(\n",
    "    stacked_df.filter(\n",
    "        (pl.col(\"metric_name\") != \"h2h\")\n",
    "        & (pl.col(\"weight\") > 0)\n",
    "        & (pl.col(\"check_be_bid\").is_not_null())\n",
    "    ),\n",
    "    group_cols=[\"check_be_bid\", \"metric_name\"],\n",
    "    weight_col=\"weight\",\n",
    "    h2h_mode=True,\n",
    ")\n",
    "\n",
    "h2h_adv_df = compute_mti(\n",
    "    h2h_metric_df.filter(pl.col(\"adv_weight\") > 0),\n",
    "    group_cols=[\"check_be_bid\"],\n",
    "    weight_col=\"adv_weight\",\n",
    "    h2h_mode=True,\n",
    ")\n",
    "\n",
    "h2h_metric_cols = (\n",
    "    base_group_cols\n",
    "    + [\"trait\"]\n",
    "    + bebid_data_cols\n",
    "    + check_bebid_data_cols\n",
    "    + base_metric_cols\n",
    ")\n",
    "\n",
    "h2h_metric_df = pl.concat(\n",
    "    [\n",
    "        stacked_df.filter(\n",
    "            (pl.col(\"check_be_bid\").is_not_null())\n",
    "        ).select(h2h_metric_cols),\n",
    "        h2h_metric_df.select(h2h_metric_cols),\n",
    "        h2h_adv_df.select(h2h_metric_cols),\n",
    "    ],\n",
    "    how=\"vertical\",\n",
    ")\n",
    "\n",
    "h2h_metric_df.write_csv('notebook_output/h2h_metric_df.csv')\n",
    "\n",
    "agg_trait_df = compute_mti(\n",
    "    stacked_df.filter(\n",
    "        (pl.col(\"metric_name\") != \"h2h\") & (pl.col(\"weight\") > 0)\n",
    "    ),\n",
    "    group_cols=[\"trait\", \"metric_name\"],\n",
    "    weight_col=None,\n",
    "    h2h_mode=False,\n",
    ")\n",
    "\n",
    "agg_metric_df = compute_mti(\n",
    "    agg_trait_df,\n",
    "    group_cols=[\"metric_name\"],\n",
    "    weight_col=\"weight\",\n",
    "    h2h_mode=False,\n",
    ")\n",
    "\n",
    "agg_adv_df = compute_mti(\n",
    "    agg_metric_df,\n",
    "    group_cols=[],\n",
    "    weight_col=\"adv_weight\",\n",
    "    h2h_mode=False,\n",
    ")\n",
    "\n",
    "agg_metric_cols = base_group_cols + bebid_data_cols + base_metric_cols\n",
    "\n",
    "agg_metric_df = pl.concat(\n",
    "    [\n",
    "        agg_metric_df.select(agg_metric_cols),\n",
    "        agg_adv_df.select(agg_metric_cols),\n",
    "    ],\n",
    "    how=\"vertical\",\n",
    ")\n",
    "\n",
    "agg_metric_df = agg_metric_df.with_columns(\n",
    "    pl.when((pl.col(\"pctchk\") >= 0) & (pl.col(\"pctchk\") < 1000))\n",
    "    .then(pl.col(\"pctchk\"))\n",
    "    .otherwise(pl.lit(-1)),\n",
    "    pl.when((pl.col(\"statistic\") > -99) & (pl.col(\"statistic\") < 99))\n",
    "    .then(pl.col(\"statistic\"))\n",
    "    .otherwise(pl.lit(-99)),\n",
    "    pl.when(\n",
    "        (pl.col(\"metric_value\") > 1) & (pl.col(\"metric_value\") < 99)\n",
    "    )\n",
    "    .then(pl.col(\"metric_value\"))\n",
    "    .otherwise(pl.lit(0)),\n",
    ")\n",
    "\n",
    "agg_metric_df.write_csv('notebook_output/agg_metric_df.csv')\n",
    "# Generate old agg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
