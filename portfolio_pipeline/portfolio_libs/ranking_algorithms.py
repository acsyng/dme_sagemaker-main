# version=4
"""
EXAMPLE CODE TO GET RANKS:
    import dataiku
    import numpy as np
    import pandas as pd
    import ranking_algorithms
    import time

    # Dataiku dataset, read in using loadDFChunks
    ds = dataiku.Dataset("trial_pheno_metric_grouped")

    # columns that define a league. Materials with the same values for grouping_vars are placed in the same league
    grouping_vars = ['analysis_type',\
                     'market_seg','technology',\
                     'material_type','stage',\
                     'decision_group_rm','source_id','irrigation']

    # generate a list for each unique value in group_cols
    group_col = 'trait'

    # rating methods to use. Recommendation is just massey, though keener and markov are also options. NOTE: markov takes much longer to run
    # as a list since multiple methods could be used
    rating_methods = ['massey']

    # variables that define a game within a league
    # materials with the same values for common_cols (and trait) will have their score_col compared
    common_cols = ['trial_id']

    # score value
    score_col = 'result_numeric_value'

    # identifier column
    id_col='entry_id'

    # If true, score materials by taking difference in score_col. If false, use a boolean score (1 point if better)
    do_differential = False

    # If true, normalizes trait values before aggregating.
    # If do_differential is true, this should also be true otherwise the relative scale of the trait-BLUPS impacts the rating aggregation
    do_normalization = False # should be true if do_differential is true (otherwise ratings will be scaled based on trait values)

    # columns to automatically grab trait specific weights from
    weight_cols = ['mn_weight','adv_mn_weight']

    # column indicating whether each material. Default is None
    check_col = 'chkfl'

    # column indicating whether larger or smaller values are better for each trait.
    # default is None. If None, then assumes higher is better
    direction_col = 'direction'

    # extra columns to grab. 'metric' is useful, but not necessary
    extra_cols = ['metric']


    # load in data in chunks to avoid memory issues. alternatively, use generateParamIdx to generate df_params
    df_all, df_params = ranking_algorithms.loadDFChunks(ds=ds,grouping_vars=grouping_vars, id_col=id_col,\
                                group_col=group_col, check_col=check_col, direction_col=direction_col,
                                common_cols=common_cols,score_col=score_col,weight_cols=weight_cols,\
                                extra_cols=extra_cols)

    # get list of traits and weights for each trait -- could specifiy traits manually (for example: ['YGSMN','ERHTN'])
    # could also specific trait_weights manually -- {'YGSMN' : 10, 'ERHTN' : 1}
    trait_list = list(pd.unique(df_all['trait']))
    trait_weights = {}
    for trait in trait_list:
        df_trait = df_all[df_all['trait'] == trait]
        trait_weights[trait] = np.mean(df_trait['mn_weight'].values)*np.mean(df_trait['adv_mn_weight'].values)


    # get ratings and ranks
    df_out = ranking_algorithms.getRanks(df_all, df_params, group_col=group_col, group_list=trait_list, group_weights=trait_weights, \
                                            rating_methods=rating_methods, common_cols=common_cols, score_col=score_col, id_col=id_col, \
                                            check_col=check_col, direction_col=direction_col, \
                                            do_differential=do_differential,do_normalization=do_normalization)

    # Recipe outputs
    ds_out = dataiku.Dataset("trial_pheno_metric_ranked")
    ds_out.write_with_schema(df_out)
"""

"""
import packages.
"""
import pandas as pd
import numpy as np

"""
Wrapper function to get ranks for each league in df_in and df_params. This function calls other ranking functions to actually get ranks (such as getRanksAggregateListsLeague)

This function spearates the input dataframe into leagues based on column 'param_idx' and then generates a rating/ranked list for each league.
The rating/ranked list for each league is concatenated and outputted. 

Inputs:
    df_in : dataframe containing data relevant to ranking. This and df_params can be generated by loadDFChunks, or loaded normally in dataiku and via generateParamIdx
    df_params : dataframe containing meta information for each grouping (league). Comes from loadDFChunks or from generateParamIdx
    group_col : column name to define groups ('trait')
    group_list : list of groups (list of traits)
    group_weights : weight of each group in groups
    rating_methods : list of to use to generate ratings ('massey','keener','markov'). Can do a single method as ['massey'] or ['markov'] or ['keener']
    common_cols : list of column names which define a game
    score_col : column name that represents trait value
    id_col : column name of identifier for each material
    check_col : column name of whether each material is/was a check
    direction_col : column name of column indicating whether higher or lower is better for each trait. If not provided, assumes higher is better.
    do_differential : True or False, if True, take difference between score values as score. If False, give 1 point to the material with the higher score value 
    do_normalization : True or False, if True, normalize trait values for each trait. This will scale the rating vector output, which prevents large trait values
                                    from being over-prioritized when taking the difference (do_differential=True). If do_differential=False, normalizing doesn't change the output


Outputs: 
    df_out contains the ranks and ratings of each material in each league

    A rating list and a ranked list is generated for each group within a league (trait for example).
    The rating represents the predicted score difference for games between two materials. Larger ratings are better.
        If do_differential==True, then the predicted score difference is of similar magnitude to the values in score_col. 
        If do_differential==False, then the difference is related to the proportion of games one material is predicted to win against another

    The ranking is the rank-ordering of materials based on ratings. Smaller rankings are better (1 = 1st, 2 = 2nd, 3=3rd, and so on)

    Aggregated lists:
        The aggregated rating list the weighted-average of ratings across all lists.
            Since group values can be on different scales (for example, YGSMN and TWSMN), do_normalization will normalize group values 
            before aggregating. This is only useful if do_differential is also true. If both of these flags are true, the aggregated rating 
            is similar to how many standard deviations one material is from another, on average.
        The aggregated ranked list is computed in the same way. Because these processes are done separately, 
        the sorted-order of materials in the rating list doesn't necessarily correspond with the sorted-order in the ranked list
        (a material's aggregated rating can be better than another material, but its aggregated ranking is worse)


"""


def run_ranking_algorithms(
        df_in,
        score_col,
        grouping_vars
):
    """
    grouping_vars : columns that define a league. Materials with the same values for grouping_vars are placed in the same league

    group_col : generate a list for each unique value in group_cols

    ratings_methods :  rating methods to use. Recommendation is just massey, though keener and markov are also options. NOTE: markov takes much longer to run
                     as a list since multiple methods could be used

    common_cols : variables that define a game within a league. materials with the same values for common_cols (and trait) will have their score_col compared

    id_col : identifier column

    do_differential : If true, score materials by taking difference in score_col. If false, use a boolean score (1 point if better)
    do_normalization : If true, normalizes trait values before aggregating.
        If do_differential is true, this should also be true otherwise the relative scale of the trait-BLUPS impacts the rating aggregation
        should be true if do_differential is true (otherwise ratings will be scaled based on trait values)

    weight_cols : columns to automatically grab trait specific weights from

    check_col : column indicating whether each material. Default is None

    direction_col : column indicating whether larger or smaller values are better for each trait.
        default is None. If None, then assumes higher is better
        direction_col = 'direction'
    extra_col : extra columns to grab. 'metric' is useful, but not necessary
    """
    group_col = 'ap_data_sector'
    rating_methods = ['massey']
    common_cols = ['source_id']
    id_col = 'be_bid'
    do_differential = True
    do_normalization = False
    # should be true if do_differential is true (otherwise ratings will be scaled based on trait values)
    weight_cols = []
    check_col = None
    direction_col = None
    extra_cols = []

    df_out = None
    df_pre_rank, df_params = generateParamIdx(
        df_in,
        grouping_vars=grouping_vars,
        id_col=id_col,
        group_col=group_col,
        check_col=check_col,
        direction_col=direction_col,
        common_cols=common_cols,
        score_col=score_col,
        weight_cols=weight_cols,
        extra_cols=extra_cols
    )
    df_pre_rank = df_pre_rank.dropna(subset=score_col)

    if df_pre_rank.shape[0] > 0:
        # get ratings and ranks
        group_list = list(pd.unique(df_pre_rank[group_col]))
        group_weights = {}
        for grp in group_list:
            group_weights[grp] = 1

        df_out = getRanks(
            df_pre_rank,
            df_params,
            group_col=group_col,
            group_list=group_list,
            group_weights=group_weights,
            rating_methods=rating_methods,
            common_cols=common_cols,
            score_col=score_col,
            id_col=id_col,
            check_col=check_col,
            direction_col=direction_col,
            do_differential=do_differential,
            do_normalization=do_normalization
        )

    return df_out


def getRanks(df_in, df_params, group_col, group_list, group_weights, \
             rating_methods, common_cols, score_col, id_col, check_col=None, direction_col=None, do_differential=False,
             do_normalization=False):
    # deal with inputs
    if not isinstance(rating_methods, list):
        rating_methods = [rating_methods]

    # use groupby now to split by grouping vars, which was previously converted to param_idx
    df_all_leagues = df_in.groupby(by='param_idx')
    df_out_list = []
    np_to_concat_list = []  # store df's to be concatenated, then concat all at once. This is faster than doing the concat within the for loop

    # wrapper function to get ranks
    ranking_func = getRanksAggregateListsLeague

    # normalize values for each trait across all data. Do this before grouping
    # if doing differential, then this scales the ratings across traits. If not doing differential, this does nothing useful
    norm_stds = None
    if do_normalization == True:
        norm_stds = {}
        for group in group_list:
            mask = df_in[group_col] == group
            norm_stds[group] = np.std(df_in.loc[mask, score_col])

    # get ranks for each method and each set of params
    for method in rating_methods:
        for group_criteria, df_league in df_all_leagues:  # grouped by league
            np_ranks_agg, other_out = ranking_func(df=df_league, \
                                                   group_col=group_col, group_list=group_list,
                                                   group_weights=group_weights, \
                                                   rating_method=method, common_cols=common_cols, score_col=score_col, \
                                                   id_col=id_col, check_col=check_col, direction_col=direction_col,
                                                   do_differential=do_differential, norm_stds=norm_stds)

            if np_ranks_agg.shape[0] > 0:
                # store param idx
                np_ranks_agg = np.hstack((np_ranks_agg, group_criteria * np.ones((np_ranks_agg.shape[0], 1))))
                np_to_concat_list.append(np_ranks_agg)

        # concat all df_ranks_agg at once
        # df_out = pd.concat(df_to_concat_list, axis=0)
        if len(np_to_concat_list) > 0:
            np_out = np.vstack(np_to_concat_list)

            # convert np_out to a dataframe, then do merge with parameters for final output.
            df_out = pd.DataFrame(np_out,
                                  columns=[id_col + 'x', 'rating', 'ranking', id_col, 'weight', 'trait', 'was_check',
                                           'norm_std', 'param_idx'])  # make sure param_idx is last in this list
            df_out['param_idx'] = pd.to_numeric(df_out['param_idx'])
            # merge meta info from params back into df_out
            df_params['param_idx'] = pd.to_numeric(
                df_params['param_idx'])  # make sure param_idx is an int, not an object
            df_out = df_out.merge(df_params, on='param_idx')
            df_out = df_out.drop(columns='param_idx')

            df_out['method'] = method
            df_out_list.append(df_out)

            # concat dataframes across methods. Method column will specify name of method
    if len(df_out_list) > 0:
        df_out = df_out_list[0]
        for i in range(1, len(df_out_list)):
            df_out = pd.concat((df_out, df_out_list[i]))
        return df_out
    else:
        return pd.DataFrame()


"""
Function to aggregate trait values via lists. The intention is to call getRanks, which then calls this function for each league

This function performs the following steps:
    1. generates a ranked list for each group in group_col (for example, each trait) via the provided rating method
        a. gets scores of "games" played between materials. A "game" is played if the two materials share the same values in common_cols (such as 'trial_id' and/or 'irrigation')
                Uses the value in score_col to compare performance of materials. direction_col can provide directionality
        b. generates scoring matrix, A, using all games played. A_ij contains how many points material i scores against material j across all games
                Also generates matrix M, which contains how many games are played between materials
        c. generates rating vector using the provided rating method
        d. generates ranking vector from the rating vector

    2. aggregates all ranked lists from each group in group col (for example, ranked lists for each trait) into an aggregated rating and ranking list
        a. This takes the weighted-average of ratings and the weighted-average of rankings across ranked lists to generate a single list
        b. because these processes are done separately, the sorted-order of materials in the rating list doesn't necessarily correspond with the sorted-order in the ranked list
             (a material can be rated better than another material, but ranked worse)
    3. Outputs each individual rating/ranked list and the aggregated lists in the 'tall' format.



"""


def getRanksAggregateListsLeague(df, group_col, group_list, group_weights, \
                                 rating_method, common_cols, score_col, id_col, \
                                 check_col=None, direction_col=None, do_differential=True, norm_stds=None):
    # initialize some variables
    game_arr = np.array([])
    weight_arr = np.array([])
    score_dir_arr = np.array([])

    rank_list_to_concat = []

    # convert some columns to integers because integer comparison is faster than string comparison
    df = convertColToIntegerIdx(df, col=id_col, new_col=id_col + 'x')
    for col in common_cols:
        df = convertColToIntegerIdx(df, col=col, new_col=col + 'x')
    uniq_idxs = pd.unique(df[id_col + 'x'])
    uniq_entries = pd.unique(df[id_col])

    # group by group_col (such as trait)
    df_grouped = df.groupby(by=group_col)
    # for each group in group_col, rank unique id_col entries (such as materials)
    for group_criteria, df_group in df_grouped:
        if group_criteria in group_list:  # if this is a group we care about, make list. Otherwise skip
            # check whether bigger is better. if smaller is better, then flip sign

            score_dir = 1  # 1 means higher is better. This is multiplied in generateScoreMatrix and deals with directionality of each trait.
            if df_group.shape[0] > 0 and direction_col is not None and direction_col in df_group.columns and \
                    df_group[direction_col].iloc[0] == 'right' and \
                    (score_col == 'prediction' or score_col == 'result_numeric_value'):
                score_dir = -1  # -1 means lower is better

            # get scores played by id's in this league
            game_arr_group = getGameScores(df_group, common_cols=common_cols, score_col=score_col, id_col=id_col)

            # if there are games, make a ranked list. if no games, do nothing
            if game_arr_group.shape[0] > 0:
                # get weight to use and score dir, but as matrices which is easier to use later
                weight_to_append = 0
                if group_criteria in group_weights:
                    weight_to_append = group_weights[group_criteria]

                weight_arr = weight_to_append * np.ones((game_arr_group.shape[0],))
                score_dir_arr = score_dir * np.ones_like(weight_arr)

                # get score matrix for this group, from which ratings can be derived
                A, M = generateScoreMatrix(game_arr_group, uniq_idxs, weights=None, score_dir=score_dir_arr, \
                                           score_col=score_col, id_col=id_col, do_differential=do_differential)

                # use score matrix to get ratings,
                if rating_method.lower() == "massey":  # Massey's method
                    r, method_out = getMasseyRating(A, M)
                elif rating_method.lower() == "markov":  # Markov's method
                    r, method_out = getMarkovChainRating(A, M)
                elif rating_method.lower() == 'keener':  # Keener's method
                    r, method_out = getKeenerRating(A)
                else:
                    raise Exception('Provided method is not known. Use one of these: massey, keener, markov')

                # make a ranked list from the rating
                np_ranks = makeRankFromRatingNumpy(r, uniq_idxs, uniq_entries, A=A, larger_better=False)

                # concat rank and rating with some useful info:
                # trait and normalization factor
                trait_arr = np.array([group_criteria]).repeat(np_ranks.shape[0]).reshape((-1, 1))
                norm_std = 1
                if norm_stds is not None and r.shape[0] > 0:
                    norm_std = np.std(r)

                # get whether each entry was a check
                if check_col is not None:
                    df_is_check = df_group[[id_col + 'x', id_col, check_col]].groupby(by=id_col + 'x').first()
                    check_names = df_is_check[df_is_check[check_col] == 1][id_col].values
                    is_check_arr = np.any(np_ranks[:, 3].reshape((-1, 1)) == check_names.reshape((1, -1)),
                                          axis=1).astype(int)
                else:
                    is_check_arr = np.zeros(
                        (np_ranks.shape[0], 1))  # if no checks inputted, put 0 in column for all values

                # concat ranks/ratings with meta info
                np_ranks = np.hstack((np_ranks, weight_to_append * np.ones((np_ranks.shape[0], 1)), trait_arr,
                                      is_check_arr.reshape((-1, 1)), norm_std * np.ones((np_ranks.shape[0], 1))))
                # np_ranks columns are -> [id_col+'x', rating, ranking, id_col, trait_weight, trait_name, is_check, std_norm]
                # higher ratings are better
                # lower rankings are better

                rank_list_to_concat.append(np_ranks)

    if len(rank_list_to_concat) > 0:  # aggregate ranking lists
        # average ranking represents the average rank each material had across groups (traits). Smaller is better
        # average rating represents the average number of standard deviations each material is from mean (0). Larger is better
        # average ranking does not necessarily have the same order as average rating, since they are computed separately.

        np_to_group = np.vstack(rank_list_to_concat)

        # aggregate across everything for each entry idx, compute average weighted rating and ranking per entry idx
        uniq_idx_grouped, uniq_idx, uniq_counts = np.unique(np_to_group[:, 0], return_inverse=True, return_counts=True)
        sort_idx = np.argsort(uniq_idx)
        uniq_idx = uniq_idx[sort_idx]
        np_sort = np_to_group[sort_idx, :]

        ranking_grouped = np.split(np_sort[:, 2], np.cumsum(uniq_counts))
        rating_grouped = np.split(np_sort[:, 1], np.cumsum(uniq_counts))
        weight_grouped = np.split(np_sort[:, 4], np.cumsum(uniq_counts))
        is_check_grouped = np.split(np_sort[:, 6], np.cumsum(uniq_counts))
        norm_std_grouped = np.split(np_sort[:, 7], np.cumsum(uniq_counts))

        avg_ranking = np.zeros((uniq_idx_grouped.shape[0],))
        avg_rating = np.zeros((uniq_idx_grouped.shape[0],))
        was_check = np.zeros((uniq_idx_grouped.shape[0], 1))
        for i in range(uniq_idx_grouped.shape[0]):
            # take weighted mean, normalize by standard dev of trait values if requested (norm_std_grouped will be 1 otherwise)
            if np.sum(weight_grouped[i]) > 0:
                avg_ranking[i] = np.dot(weight_grouped[i], ranking_grouped[i]) / np.sum(weight_grouped[i])
                avg_rating[i] = np.dot(weight_grouped[i], np.divide(rating_grouped[i], norm_std_grouped[i])) / np.sum(
                    weight_grouped[i])  # rescale ratings and take average
            else:
                avg_ranking[i] = np.mean(ranking_grouped[i])
                avg_rating[i] = np.mean(
                    np.divide(rating_grouped[i], norm_std_grouped[i]))  # rescale ratings and take average
            # if check for any trait, set as 1
            was_check[i] = np.any(is_check_grouped[i] == 1)

        # package avg_rankings and individual trait rankings
        # np_ranks_agg is [id_col+'x', rating, ranking, id_col, weight, trait, was check]
        np_avg_ranks = np.vstack(
            (uniq_idx_grouped, avg_rating, avg_ranking, uniq_entries[uniq_idx_grouped.astype(int)])).T

        # add trait ('aggregate') and weight col and std_col (1 for both, as this is meaningless for aggregated trait)
        # add check flag col -- if it was a check for any trait, put a 1
        trait_arr = np.array(['aggregate']).repeat(np_ranks.shape[0]).reshape((-1, 1))
        np_avg_ranks = np.hstack(
            (np_avg_ranks, np.ones((np_ranks.shape[0], 1)), trait_arr, was_check, np.ones((np_ranks.shape[0], 1))))

        np_out = np.vstack((np_avg_ranks, np_to_group))

        return np_out, (uniq_idxs)
    else:
        return np.array([]), (df_grouped)


"""
Function to get the scores of games played between materials. 
Function should be called for each league. Currently called for each trait individually
Uses columns within common_cols to define games
The output of this function is used to generate a scoring matrix (A) by calling generateScoreMatrix

Inputs: df : dataframe containing data for a league
    common_cols : list of column names which define a game. Assumes higher score is better. Flip values before calling function if lower values are better
    score_col : column name which holds score value
    id_col : column name to identify materials

Outputs : game_arr : numpy array holding game scores  -- columns are [idcol1, idcol2, score1, score2].

"""


def getGameScores(df, common_cols=['trial_id'], score_col='prediction', id_col='entry_id'):
    # check to see if df has columns entry_idx, and common_cols+'x', generated by convertColToIntegerIdx
    if id_col + 'x' not in df.columns:
        df = convertColToIntegerIdx(df, col=id_col, new_col=id_col + 'x')
    for col in common_cols:
        if col + 'x' not in df.columns:
            df = convertColToIntegerIdx(df, col=col, new_col=col + 'x')

    # materials play 'games' against each other when the values in common_cols are the same
    # for example, same source id and check entry id
    common_cols_adj = [col + 'x' for col in common_cols]
    # get a list of unique 'fields' where games are played
    df_uniq_fields = df.drop_duplicates(subset=common_cols_adj)

    # aggregate over duplicated entry_idx and common cols
    # perform group by now instead of within for loop
    groupby_cols = [id_col + 'x']
    groupby_cols.extend(common_cols_adj)  # common cols is a list
    agg_dict = {}
    for col in common_cols:
        agg_dict[col] = 'first'
    agg_dict[score_col] = 'mean'
    df_group = df.groupby(by=groupby_cols).agg(agg_dict).reset_index()

    flag_first = 1
    df_fields = df_group.groupby(by=common_cols_adj)
    game_arr = np.array([])
    for x, df_field in df_fields:
        # get games played on this field
        # get all pairwise comparisons, store entry id's, meta info, scores

        entry1_id = getRepeatedMatrix(df_field[id_col + 'x'].values, n_reps=df_field.shape[0], axis=1)
        entry2_id = getRepeatedMatrix(df_field[id_col + 'x'].values, n_reps=df_field.shape[0], axis=0)
        score1 = getRepeatedMatrix(df_field[score_col].values, n_reps=df_field.shape[0], axis=1)
        score2 = getRepeatedMatrix(df_field[score_col].values, n_reps=df_field.shape[0], axis=0)

        temp_game_arr = np.stack((entry1_id, entry2_id, score1, score2), axis=1)
        if flag_first == 1:
            game_arr = temp_game_arr
            field_arr = np.ones((game_arr.shape[0],))
            flag_first = 0
        else:
            game_arr = np.append(game_arr, temp_game_arr, axis=0)

    # game arr is idcol1, idcol2, score1, score2
    return game_arr


"""
Generate scoring matrix A and games played matrix M 
A_ij represents the score of team i vs team j, assuming that team i won
M_ij represents the number of games played by team i against team j, M_ii is be all 0's at this point


Inputs:
    game_arr : numpy array each row contains the id of the two materials being compared and their scores ---- generated by getGamesScores 
            game_arr columns are [idcol1, id_col2, score1, score2]
    uniq_idxs : 
    weights : Nx1 vector containing the weight of each game. If None, then no weighting is done
    score_dir : Nx1 vector containing 1 and -1. Represents whether higher is better (1) or if lower is better (-1)
    score_col : column name which holds score value
    id_col : column name to identify materials
    do_differential : True or False, if True, take difference between score values as score. If False, give 1 point to the material with the better score value

Outputs:
    A and M (described above)

"""


def generateScoreMatrix(game_arr, uniq_idxs, weights=None, score_dir=None, score_col='prediction', id_col='entry_id',
                        do_differential=True):
    # game_arr is idcol1, idcol2, score1, score2
    # weights and score_dir are an Nx1 vector (N is game_arr.shape[0])
    # initialize matrix A, a_ij represents the score of team i against team j in a game
    # initialize matrix M, m_ij represents the number of games played by team i against team j.
    # do_differential = True outputs score differentials
    # do_differential = False just counts which entry wins each game, each game referring to traits
    if score_dir is None:  # initialize as default
        score_dir = np.ones((game_arr.shape[0], 1))

    # initialize matrix A, a_ij represents the score of team i against team j in a game
    # initialize matrix M, m_ij represents the number of games played by team i against team j.
    # do_differential = True outputs score differentials
    # do_differential = False just counts which entry wins each game, each game referring to traits

    A = np.zeros((uniq_idxs.shape[0], uniq_idxs.shape[0]))
    M = np.zeros_like(A)

    # get score diff and ab, weight if column in df_games
    # default is to measure the difference between score columns
    score_diff = np.multiply(score_dir, game_arr[:, 2] - game_arr[:, 3])
    game_weight = np.ones((score_diff.shape[0],))
    # if not differential, just use boolean output of who wins
    if do_differential == False:
        score_diff[score_diff > 0] = 1
        score_diff[score_diff < 0] = -1

    if weights is not None:
        score_diff = np.multiply(score_diff, weights)
        game_weight = np.multiply(game_weight, weights)

    # get idx to update if score1 is larger
    update_idx_pos = np.array([(x, y) for x, y in zip(game_arr[:, 0], game_arr[:, 1])], dtype="i,i")
    # flip the indices if score diff is below 0
    update_idx_neg = np.array([(y, x) for x, y in zip(game_arr[:, 0], game_arr[:, 1])], dtype="i,i")
    update_idx = np.where(score_diff > 0, update_idx_pos, update_idx_neg)
    # update only (winner_idx, loser_idx), not (loser_idx, winner_idx), so all scores are positive values. take absolute value
    score_diff = np.abs(score_diff)

    # get all scores for each update_idx using split, then sum in the for loop
    # this involves getting the unique update locations, and some weird code to use split
    # split is doing a pandas.groupby(by=update_idx), but faster because it's numpy
    # the sum is done in the for loop
    uniq_update, uniq_idx, uniq_counts = np.unique(update_idx, return_inverse=True, return_counts=True)
    sort_idx = np.argsort(uniq_idx)
    uniq_idx = uniq_idx[sort_idx]
    score_diff_sort = score_diff[sort_idx]
    game_weight_sort = game_weight[sort_idx]

    score_diff_grouped = np.split(score_diff_sort, np.cumsum(uniq_counts))
    game_weight_grouped = np.split(game_weight_sort, np.cumsum(uniq_counts))

    for i in range(uniq_update.shape[0]):  # this is pretty fast now that it's only doing updates
        A[uniq_update[i][0], uniq_update[i][1]] = np.sum(score_diff_grouped[i])

        M[uniq_update[i][0], uniq_update[i][1]] = M[uniq_update[i][0], uniq_update[i][1]] + np.sum(
            game_weight_grouped[i])
        M[uniq_update[i][1], uniq_update[i][0]] = M[uniq_update[i][1], uniq_update[i][0]] + np.sum(
            game_weight_grouped[i])

    return A, M


##############################################
#### METHODS TO GET RANKING/RATING VECTOR.####
##############################################


"""
Function to get rating using Massey's method. Massey's method involves solving the linear system M*r=p
    M is a matrix representing the games played between teams. 
    This matrix is derived from the one inputted into the function.
    M_ij = -1*#games played between teams i and j where i != j
    M_ii = # games played by team i
    p is a column vector containing the total point differentials across all games played by team i
    To avoid linear regression issues, Massey recommends setting the last row of M to all ones and the last entry in p to 0

Inputs:
    A : stores points scored by one team against another. 
        A_ij represents the score of team i vs team j, assuming that team i won. If team i lost, then a 0
    M : stores # of games played between two teams as positive integers. Diagonal is all 0's
        M_ij represents the number of games played by team i against team j AS A POSITIVE NUMBER. M_ii can be all 0's at this point
        I suppose M_ii can be anything, the function sets it to the correct values based on all M_ijs

Outputs:
      massey rating (higher is better)
      M and p matrices/vectors used to generate the rating vector (as a tuple to be consistent with other rating function outputs)
"""


def getMasseyRating(A, M):
    # setup the massey p and M
    massey_p = np.sum(A, axis=1) - np.sum(A, axis=0)
    # make off diagonals of M as negative numbers, diagonals are number of games played by each team
    massey_M = M * -1
    np.fill_diagonal(massey_M,
                     -1 * np.sum(massey_M, axis=1) + 1E-9)  # avoid singularity by adding small number to diagonal

    # replace last row of M with all ones, replace last entry of p with a 0
    massey_p[-1] = 0
    massey_M[-1, :] = 1
    # solve linear system for r
    massey_r = np.linalg.solve(massey_M, massey_p)

    return massey_r, (massey_M, massey_p)


"""
Function to get rating using Markov's method. Markov's method involves each material voting for strong materials
for the markov method, we do a random walk on a graph generated by games
each node is a team, and the probability of stepping from team i to team j is based on the scores of games between those two teams
If we are at node i, then the probability of going to node j should be high if j beats i by a lot
In this way, team i votes for team j a lot because team j beat team i by a lot
use the columns of A as voting probabilities, not the rows (since we want p_ij to be high if team j beats team i, not the other way)

Inputs:
    A : stores points scored by one team against another. 
        A_ij represents the score of team i vs team j, assuming that team i won. If team i lost, then a 0
    M : stores # of games played between two teams as positive integers. Diagonal is all 0's
        M_ij represents the number of games played by team i against team j AS A POSITIVE NUMBER. M_ii can be all 0's at this point
        I suppose M_ii can be anything, the function sets it to the correct values based on all M_ijs

Outputs:
    markov rating (higher is better)
    Voting matrix, n_reach (number of times at each team)
"""


def getMarkovChainRating(A_in, M_in, n_steps=50000, n_walkers=2000):
    # This code was originally writen to use rows as voting probabilities, so we transpose the A matrix immediately.
    # normalize scores based on number of games played against each other
    A = A_in.copy()
    M = M_in.copy()
    np.fill_diagonal(M, 1)
    A = np.divide(A, M)
    A = A.T
    # normalize cols of A to represent a probability. If row is all 0's replace with uniform distribution
    A[np.all(A == 0, axis=1), :] = 1
    A = np.divide(A, np.sum(A, axis=1).reshape((-1, 1)))

    # perform random walk
    pos = np.random.choice(range(A.shape[0]), n_walkers)
    rand_nums = np.random.rand(n_steps, n_walkers)
    A_cum_sum = np.cumsum(A, axis=1)

    n_reach = np.zeros((A.shape[0],))
    for i_step in range(n_steps):
        # move each walker based on row i in M
        pos = np.sum(np.repeat(rand_nums[i_step, :].reshape((-1, 1)), A_cum_sum.shape[1], axis=1) > A_cum_sum[pos, :],
                     axis=1)
        # update n_reach by counting each unique element in pos
        unique_pos, counts_pos = np.unique(pos, return_counts=True)
        n_reach[unique_pos] = n_reach[unique_pos] + counts_pos

    # higher ratings are better
    rating_prob = n_reach / np.sum(n_reach)
    return rating_prob, (A, n_reach)


"""
Keener's method
generate matrix A (m x m, m is number of teams)
where each entry a_i,j is the value of a statistic 
produced by team i when competing against team j
example: wins + 0.5*ties or offensive yards in football
    points i scores against j

Keener recommends some adjustments instead of just using raw scores:
a_ij = (S_ij + 1)/(S_ij + S_ji + 2) -- normalize by total score of both teams
this accounts for high scoring/low scoring contests
the +1 and +2 makes is so that if S_ij or S_ji are 0, the output is not just 0 or 1
also as the difference S_ij - Sji increases, a_ij approahces 1

may need to skew a_ij's and/or normalize based on number of games played
be careful of over-nomalizing the data

Keener says that strength (how well a team performs against another team)
should be proportional to rating: 
matrix A represents how well teams perform against other teams
Keener derives the following equation: A*r = lambda*r
rating is an eigenvector of A
Constraints on A:
A must be nonnegative, A must irreducible (teams i and j must be connected by a series of games)
 Primitivity requires that all teams must be connected by the same number of games

Can force irreducibility/primitivity by adding a small number to all values of A
if we have irreducibility, could also add a small value to a single diagonal element
to guarantee primitivity

IF added epsilon to a single diagonal element of A -- if matrix is irreducible
r = [A + eps*e*e']*r/(e'[A+eps*e_i*e_i']r)
where e is a column matrix of all ones, e_i is a columns matrix of all 0's except for a 1 at index i

so algorithm is as follows: generate A and normalize/skew
force irreducibility/primitivity by adding epsilon
repeatedly compute r until convergence

outputs rating and adjusted A
"""


def getKeenerRating(A_in):
    # normalize A by (Aij+1)/(Aij+Aji+2)
    A = np.divide(A_in + 1, A_in + A_in.T + 2)
    r = 1 / A.shape[0] * np.ones((A.shape[0], 1))  # initialize r with all 1/m
    eps = 1E-10

    for i in range(200):  # repeat until converge to a prescried number of significant digits
        # IF added epsilon to all elements of A to force irredicubility and primitivity
        # r = [A + eps*e*e']*r/(e'[A+eps*e*e']r)
        num = (A + eps) @ r
        denom = np.ones((1, A.shape[0])) @ num
        r = num / denom
    return r.reshape((-1,)), (A)


###################################################################
#### FUNCTIONS TO LOAD IN DATAIKU DATA AND TO SEPARATE LEAGUES ####
###################################################################

"""
Function to put league indicator into dataframe. Materials get ranked within each "league," 
and param_idx indicates which league the corresponding row in df_out refers to (param_idx == 1 is a "league", param_idx == 2 is a "league" and so on).
Splitting the data initially should reduce total memory size, since the meta information is only stored once. Memory was an issue when running on CORN_NA_SUMMER,
    though maybe this is unnecessary.

This function is used if loadDFChunks is not used. 

Returns dataframe with group idx and all group columns removed, as well as a dataframe with grouping information
Output dataframes can be merged using param_idx column

Inputs: 
    df : pandas dataframe
    grouping_vars : list of columns that define a "league" for ranking (ex. ['analysis_type','market_seg','technology','material_type','stage','decision_group_rm','irrigation','source_id'])
    id_col : column name of material identifier (ex. 'entry_id')
    common_cols : list of column names that define a game (ex. 'trial_id')
    score_col : column name for trait values (ex. 'dme_mean_metric' or 'result_numeric_value')
    weight_cols : list of column names that define the weight for each trait
    extra cols : list of extra column names to grab. trait and param_idx need to be in the list. direction is also useful.

Outputs:
    df_all : dataframe containing data needed to perform ranking analysis
    df_params : dataframe containing parameter information for each group. Can be merged with df_all using param_idx 
"""


def generateParamIdx(df, grouping_vars=['ap_data_sector'], id_col='entry_id', group_col='trait', check_col=None,
                     direction_col=None, common_cols=['trial_id'], \
                     score_col='dme_mean_metric', weight_cols=['mn_weight', 'adv_mn_weight'], extra_cols=['metric']):
    df_params = df[grouping_vars].drop_duplicates()
    df_params['param_idx'] = np.arange(0, df_params.shape[0])
    df_out = df.merge(df_params, on=grouping_vars)  # put param idx into original df

    # only keep certain columns from original dataframe.
    cols_keep = common_cols.copy()
    cols_keep.append('param_idx')
    cols_keep.append(id_col)
    cols_keep.append(group_col)
    cols_keep.append(score_col)

    if weight_cols is not None:
        if isinstance(weight_cols, list) and len(weight_cols) > 0:
            cols_keep.extend(weight_cols)
        elif not isinstance(weight_cols, list):
            cols_keep.append(weight_cols)

    if check_col is not None:
        cols_keep.append(check_col)
    if direction_col is not None:
        cols_keep.append(direction_col)
    if extra_cols is not None:
        if isinstance(extra_cols, list) and len(extra_cols) > 0:
            cols_keep.extend(extra_cols)
        elif not isinstance(weight_cols, list):
            cols_keep.append(extra_cols)

    df_out = df_out[cols_keep]
    return df_out, df_params


"""
This is a dataiku specific function that loads data in in chunks to prevent memory overflow
Returns dataframe with relevant data for ranking algorithms as well as a parameter dataframe that stores meta data
The param_idx column can be used to join the two outputs. Param idx can also be used to iterate through different groupings

Inputs: 
    ds : dataiku dataset (example: dataiku.Dataset("temp"))
    grouping_vars : list of columns that define a "league" for ranking (ex. ['analysis_type','market_seg','technology','material_type','stage','decision_group_rm','irrigation','source_id'])
    id_col : column name of material identifier (ex. 'entry_id')
    check_col : column name of column indicating whether a material is/was a check
    direction_col : column name of column indicating whether a higher or lower trait value is better. If none, assumes higher is better
    common_cols : list of column names that define a game (ex. 'trial_id')
    score_col : column name for trait values (ex. 'dme_mean_metric' or 'result_numeric_value')
    weight_cols : list of column names that define the weight for each trait
    extra cols : list of extra column names to grab. trait and param_idx need to be in the list. direction is also useful.

Outputs:
    df_all : dataframe containing data needed to perform ranking analysis
    df_params : dataframe containing parameter information for each group. Can be merged with df_all using param_idx

"""
"""
def loadDFChunks(ds,grouping_vars=['ap_data_sector'],id_col='entry_id',group_col='trait',check_col=None, direction_col=None, common_cols=['trial_id'], \
                 score_col='dme_mean_metric', weight_cols=['mn_weight','adv_mn_weight'], extra_cols=['metric']):
    # initialize dataframe to hold relevant data
    # gather useful columns
    df_all_cols = common_cols.copy()
    df_all_cols.append(score_col)

    df_all_cols.extend(weight_cols)
    df_all_cols.append(id_col)
    df_all_cols.append(group_col)

    if check_col is not None:
        df_all_cols.append(check_col)
    if direction_col is not None:
        df_all_cols.append(direction_col)

    if extra_cols is not None: 
        if isinstance(extra_cols, list) and len(extra_cols) > 0:
            df_all_cols.extend(extra_cols)
        else:
            df_all_cols.append(extra_cols)

    # append param_idx to df_all_cols manually. This is an output of this function
    df_all_cols.append('param_idx')

    # initialize dataframe with correct columns
    df_all = pd.DataFrame(columns=df_all_cols)

    # initialize dataframe to hold params for each "league"
    param_cols = grouping_vars.copy()
    param_cols.append('param_idx')
    df_params = pd.DataFrame(columns=param_cols)

    # columns to check for nan's
    nan_cols = common_cols.copy()
    nan_cols.append(score_col)

    chunksize=1000000
    for df_chunk in ds.iter_dataframes(chunksize=chunksize):
        # grab all relevant data and store
        # drop rows where common cols or score col are nan
        df_chunk = df_chunk.dropna(subset=nan_cols)

        # first make sure there are no new unique sets of grouping vars
        df_params_temp = df_chunk[grouping_vars].drop_duplicates()
        max_param_idx=-1 # we add one when making the column, so this will start at 0
        if df_params.shape[0] > 0:
            max_param_idx = np.max(df_params['param_idx'])

        df_params_temp['param_idx'] = np.arange(0,df_params_temp.shape[0]) + max_param_idx + 1

        df_params = pd.concat((df_params, df_params_temp), axis=0)
        df_params = df_params.drop_duplicates(subset=grouping_vars)
        # keep analysis year as a number, not an object
        if 'analysis_year' in df_params.columns:
            df_params['analysis_year'] = pd.to_numeric(df_params['analysis_year'])
        # get param_idx for each row in df_chunk
        df_chunk = df_chunk.merge(df_params, on=grouping_vars)
        # extract relevant data and store
        df_all = pd.concat((df_all, df_chunk[df_all.columns]))

    return df_all , df_params  
  """


#################################
#### USEFUL HELPER FUNCTIONS ####
#################################

# convert common cols to integers to make comparisons faster
def convertColToIntegerIdx(df_in, col, new_col):
    df_out = df_in.copy()
    # only do this for non numeric entries
    if not pd.api.types.is_numeric_dtype(df_out[col]):
        # convert entry id to integers to make comparisons faster
        uniq_ids = pd.unique(df_out[col])
        entry_idx_map = {b: a for a, b in enumerate(uniq_ids)}

        df_out[new_col] = np.array([entry_idx_map[key] for key in df_out[col].values])
    return df_out


"""
This is a helper function for generating pairwise combinations quickly
This function needs to be called twice with different axis inputs: 
    score1 = getRepeatedMatrix(df_field[score_col].values, n_reps=df_field.shape[0], axis=1)
    score2 = getRepeatedMatrix(df_field[score_col].values, n_reps=df_field.shape[0], axis=0)    
This function takes in a vector (vals) and outputs the values in vals repeated many times. 
By changing the axis, the order of the output changes. If the function is called twice, as above,
then score1 and score2 will have all pairwise comparisons (score1[i] compared to score2[i])

This function works by tiling vals across all rows or column (based on axis), then outputting the lower triangular entries only
By tiling across rows for one variable and columns for the other, pairwise comparisons are outputted


ex: [A A A A]
    [B B B B]
    [C C C C]
    [D D D D]

    out = [B,C,C,D,D,D]

    [A B C D]
    [A B C D]
    [A B C D]
    [A B C D]

    out = [A,A,B,A,B,C]

    comparisons: [AB,AC,BC,AD,BD,CD]

"""


def getRepeatedMatrix(vals, n_reps, axis):
    shape = (-1, 1)
    if axis == 0:
        shape = (1, -1)
    mat = np.repeat(vals.reshape(shape), repeats=n_reps, axis=axis)
    return mat[np.tril_indices(mat.shape[0], k=-1)]


def makeRankFromRatingNumpy(r, uniq_idxs, uniq_entries, A=None,
                            larger_better=False):  # larger_better refers to output, not input
    # if A is inputted, then df_rating needs to have id_col+'x' as a column
    rating_col = 1
    ranking_col = 2

    np_out = np.zeros((uniq_idxs.shape[0], 3))
    sort_idx = np.argsort(r)
    np_out[:, 0] = uniq_idxs[sort_idx]
    np_out[:, rating_col] = r[sort_idx]

    # get ranking from sort -- ascending order, so need to flip rankings
    np_out[:, ranking_col] = np_out.shape[0] + 1 - np.arange(1, np_out.shape[0] + 1)
    np_out = np.concatenate((np_out, uniq_entries[sort_idx].reshape(-1, 1)), axis=1)

    # handle ties (poorly currently)
    flag_tie = 0
    np_out[:, 0] = np_out[:, 0].astype(int)
    for i in range(0, np_out.shape[0] - 1):
        if np_out[i, rating_col] == np_out[i + 1, rating_col]:
            flag_tie = 1
            i_wins = 0
            if A is not None:  # only compute winner in the head-to-head if A is provided
                i_wins = A[np_out[i, 0], np_out[i + 1, 0]] - A[np_out[i + 1, 0], np_out[i, 0]]

            if i_wins < 0:  # i+1 wins, swap rank_col vals
                temp = np_out[i, ranking_col]
                np_out[i, ranking_col] = np_out[i + 1, ranking_col]
                np_out[i + 1, ranking_col] = temp
            elif i_wins == 0:  # tie...set rank as mean of two
                new_rank = np.mean(np_out[i:i + 2, ranking_col])
                np_out[i, ranking_col] = new_rank
                np_out[i + 1, ranking_col] = new_rank

    return np_out


#################################################################################################
#### CODE BELOW IS NOT USED IN THE CURRENT VERSION, but still might be useful to keep around ####
#################################################################################################


"""
function to aggregate traits via score function. Called by getRanks. May not work currently since many changes were made to other functions

The idea is to aggregate traits via the score function. As a terrible example, score could be YGSMN + GMSTP + ERHTN, and then scores would be compared 
across materials. This is as opposed to making a ranked list for each of YGSMN, GMSTP, ERHTN, and then aggregating those ranked lists

The problem with this approach is information from checks gets aggregated across multiple traits, some of which the check may not be a check for.
For example, if CHECK_1 is a check for GMSTP, but it's YGSMN is also measured, the total score would include the YGSMN value, which may not be desired.

Instead, make a ranked list for each of the traits, then aggregate. This way the trait-BLUP-value for each check is used properly.
use getRanksAggregateListsLeague to do that

"""


def getRanksAggregateScoringFuncLeague(df, group_col, group_list, group_weights, rating_method, common_cols, score_col,
                                       id_col, check_col, do_differential, norm_stds=None):
    # overhead here costs next to nothing (1% of total time)
    game_arr = np.array([])
    weight_arr = np.array([])
    score_dir_arr = np.array([])

    flag_first = 1

    # get scores for each trait, then sum together before running ranking method
    df = convertColToIntegerIdx(df, col=id_col, new_col=id_col + 'x')
    for col in common_cols:
        df = convertColToIntegerIdx(df, col=col, new_col=col + 'x')
    uniq_idxs = pd.unique(df[id_col + 'x'])
    uniq_entries = pd.unique(df[id_col])

    df_grouped = df.groupby(by=group_col)

    for g, df_group in df_grouped:
        if g in group_list:
            # check whether bigger is better for the trait, which is what the ranking code assumes
            # if smaller is better, then flip sign
            # dme_metric/score already have dealt direction

            score_dir = 1  # 1 means higher is better. This is multiplied in generateScoreMatrix and deals
            # with directionality of each trait

            if df_group.shape[0] > 0 and 'direction' in df_group and df_group['direction'].iloc[0] == 'right' and \
                    (score_col == 'prediction' or score_col == 'result_numeric_value'):
                score_dir = -1  # -1 means lower is better

                # with pd.option_context('mode.chained_assignment', None):# this should suppress the copy warning
                #    df_group[score_col] = -1*df_group[score_col].values

            game_arr_temp = getGameScores(df_group, common_cols=common_cols, score_col=score_col, id_col=id_col)
            # append score dir to df_games_temp?

            # append weight column to df_games_temp, append temp dataframe to df_games
            if game_arr_temp.shape[0] > 0:
                if g in group_weights:
                    weight_to_append = group_weights[g]
                else:
                    weight_to_append = 0

                if flag_first == 1:
                    game_arr = game_arr_temp
                    weight_arr = weight_to_append * np.ones((game_arr_temp.shape[0],))
                    score_dir_arr = score_dir * np.ones((game_arr_temp.shape[0],))

                    flag_first = 0
                else:
                    game_arr = np.append(game_arr, game_arr_temp, axis=0)
                    weight_arr = np.append(weight_arr, weight_to_append * np.ones((game_arr_temp.shape[0],)), axis=0)
                    score_dir_arr = np.append(score_dir_arr, score_dir * np.ones((game_arr_temp.shape[0],)), axis=0)

    if game_arr.shape[0] > 0 and len(game_arr.shape) > 1 and game_arr.shape[
        1] > 3:  # a bunch of shape checks to make sure the games were grabbed correctly
        # get score matrix, use weight column in df_games to weight different groups
        A, M = generateScoreMatrix(game_arr, uniq_idxs, weights=weight_arr, score_dir=score_dir_arr, \
                                   score_col=score_col, id_col=id_col, do_differential=do_differential)

        # use score matrix to get ratings,
        if rating_method.lower() == "massey":  # Massey's method
            r, method_out = getMasseyRating(A, M)
        elif rating_method.lower() == "markov":  # Markov's method
            r, method_out = getMarkovChainRating(A, M)
        elif rating_method.lower() == "keener":  # Keener's method
            r, method_out = getKeenerRating(A)

        ### packaging the outputs is taking 1/5th of the total time -- numpy is faster
        np_out = makeRankFromRatingNumpy(r, uniq_idxs, uniq_entries, A=A, larger_better=False)

        ## append trait and weight columns so that this is consistent with other ranking function
        # also append check column for consistency, all 0s at this point
        # also append std column for consistency, all 1s at this point
        trait_arr = np.array(['aggregate']).repeat(np_out.shape[0]).reshape((-1, 1))
        np_out = np.hstack((np_out, np.ones((np_out.shape[0], 1)), trait_arr, np.zeros((np_out.shape[0], 1)),
                            np.ones((np_out.shape[0], 1))))

        """
        # make df with entry_id and massey rating
        df_r = pd.DataFrame(data={id_col+'x' : uniq_idxs,'rating' : r})
        df_r = df_r.merge(df[[id_col,id_col+'x']], on=[id_col+'x']).drop_duplicates()
        df_r = makeRankFromRating(df_r,id_col=id_col,rating_col='rating',A=A,larger_better=False)
        # rename ranking column since the autogenerate name is confusing
        df_r = df_r.rename(columns={'rating_ranking' : 'ranking'})
        """
        return np_out, (game_arr, A, M, uniq_idxs)
    else:
        return np.array([]), (df_grouped)


"""
Function to aggregate lists using the average ranking. 
df_ranks contains a number of ranked lists as columns (YGSMN_ranking, ERHTN_ranking)
groups is a list of variables used to generate ranked lists (typically trait)
group_weights contains the weights of each group
id_col is the material identifier column name
"""


def aggregateListsAverage(df_ranks, groups, group_weights, id_col):
    # group weights is a dictionary: e.g. {'YGSMN' : weight, 'ERHTN' :weight}
    # convert group_weights to weight vector
    weights = makeWeightVector(df_ranks.shape[1], groups, group_weights)

    df_ranks_full = df_ranks.fillna(value=0)  # need to replace NaN's with a value, use 0 because multiplying
    df_ranks_used = 1 - df_ranks.isna()  # keeps track of whether we used this value for weighted average
    # dot product over sum of weights used
    df_avg = df_ranks_full.dot(weights).divide(other=df_ranks_used.dot(weights)).rename(
        columns={0: 'avg_rating'}).astype('float64')
    df_ratings_avg = df_ranks.merge(df_avg, on=id_col).reset_index()

    # drop all columns except rating and id_col
    df_ratings_avg = df_ratings_avg[[id_col, 'avg_rating']]

    # input is a list of rankings, where smaller is better. Output needs to be larger is better, so flip rating
    with pd.option_context('mode.chained_assignment', None):  # this should suppress the copy warning
        df_ratings_avg['avg_rating'] = -1 * df_ratings_avg['avg_rating'].values

    return df_ratings_avg


# slightly more sophisticated method aggregation method: Simulate game data
# for each rank list, simulate game data between materials using ranking information
# teams higher ranked (smaller value) win by the difference in ranking
# then use a rating/ranking method (like Massey's) to rank based on this simulated game data

# simulate game data...
# assign idx to each entry id to make comparisons faster, then build score matrix for each list
# for each list, simulate all games if present. Generate A and M matrix (A is points scored, M is games played)
def aggregateListsSimulation(df_ranks, groups, group_weights, id_col):
    A = np.zeros((df_ranks.shape[0], df_ranks.shape[0]))
    M = np.zeros_like(A)

    for group in groups:
        if group in group_weights:
            weight = group_weights[group]
            rank_col = group + '_ranking'
            rank_list = -1 * df_ranks[
                rank_col].values  # we need to flip the rank list since smaller ranks are currently better
            # get point differential
            A_temp = rank_list.reshape((-1, 1)) - rank_list.reshape((1, -1))
            # remove nan entries from A, only store positive values
            A_temp[np.isnan(A_temp)] = 0
            A_temp[A_temp < 0] = 0

            # get whether teams played against each other
            M_temp = (np.isnan(A_temp) == False)

            # sum scores and games played.
            A = A + A_temp * weight
            M = M + M_temp * weight

    # put M in format for getMasseyRating(A, M)
    np.fill_diagonal(M, 0)

    agg_massey_r, (temp) = getMasseyRating(A, M)
    agg_keener_r, (temp) = getKeenerRating(A)
    agg_markov_r, (temp) = getMarkovChainRating(A, M)
    df_temp = pd.DataFrame(
        data={id_col: list(df_ranks.index), 'simMassey_rating': agg_massey_r, 'simKeener_rating': agg_keener_r,
              'simMarkov_rating': agg_markov_r})

    return df_temp


def aggregateListsBorda(df_ranks, groups, group_weights, id_col):
    # aggregate ranks by counting number of teams that have a worse ranking for each trait, weighted by weight
    # this outputs a rating, where higher score is better
    borda_r = np.zeros((df_ranks.shape[0],))

    for group in groups:
        if group in group_weights:
            weight = group_weights[group]
            rank_col = group + '_ranking'
            if rank_col in df_ranks.columns:
                borda_r = borda_r + weight * np.sum(
                    df_ranks[rank_col].values.reshape((-1, 1)) > df_ranks[rank_col].values.reshape((1, -1)), axis=0)
                # x[i,j] is x[i] > x[j]. Summing over rows will give us how many entries x[i] is bigger than.
                # We want to count how many entries x[i] is smaller than, so sum over columns not rows
                # this approach gives a higher score to smaller (better) ranks

    df_temp = pd.DataFrame(data={id_col: list(df_ranks.index), 'borda_rating': borda_r})
    return df_temp


# refine after aggregation by reordering pairs where one beats the other
# this really only works if the two entry id's have data for the same traits....
# this is also currently slow
def refineAggregatedRankings(df_ranks, agg_col='avg_ranking', other_cols=['YGSM_ranking', 'ERHTN_ranking']):
    # for each pair in agg_list (1st vs 2nd, 2nd vs 3rd, etc.)
    # does the worse-ranked team beat the better-ranked team
    # in the majority of other_lists? If yes, swap
    # should be n-1 comparisons, where n is the length of agg_list

    # df ranks contains ranking (or rating) information
    df_ranks_sort = df_ranks.sort_values(by=agg_col, ascending=True)  # smaller is better

    for i in range(df_ranks_sort.shape[0] - 1):
        worse_wins = df_ranks_sort[other_cols].iloc[i + 1] < df_ranks_sort[other_cols].iloc[i]  # smaller is better
        both_not_nan = (df_ranks_sort[other_cols].iloc[i + 1].isna() == False) & (
                df_ranks_sort[other_cols].iloc[i].isna() == False)
        prop_worse_wins = np.sum(worse_wins[both_not_nan]) / np.sum(both_not_nan)

        if prop_worse_wins > 0.5:
            # switch avg_rankings
            temp_r = df_ranks_sort[agg_col].iloc[i]
            df_ranks_sort[agg_col].iloc[i] = df_ranks_sort[agg_col].iloc[i + 1]
            df_ranks_sort[agg_col].iloc[i + 1] = temp_r

    return df_ranks_sort


# generate rankings for each rating as well (1st, 2nd, 3rd, etc...)
# normalize rankings by number of entries. Smaller number is better for rankings, larger number is better for ratings
def makeRankFromRating(df_rating, id_col, rating_col, A=None,
                       larger_better=False):  # larger_better refers to output, not input
    # if A is inputted, then df_rating needs to have id_col+'x' as a column

    rank = df_rating.sort_values(by=rating_col, ascending=larger_better).reset_index().drop(columns='index')
    rank_col = rating_col.split('_')[0] + '_ranking'
    rank[rank_col] = np.arange(0, df_rating.shape[0])
    rank[rank_col] = rank[rank_col] / rank.shape[0]

    # handle ties (poorly currently)
    flag_tie = 0
    for i in range(0, rank.shape[0] - 1):
        if rank[rating_col].iloc[i] == rank[rating_col].iloc[i + 1]:
            flag_tie = 1
            i_wins = 0
            if A is not None:  # only compute winner in the head-to-head if A is provided
                i_wins = A[rank[id_col + 'x'].iloc[i], rank[id_col + 'x'].iloc[i + 1]] - A[
                    rank[id_col + 'x'].iloc[i + 1], rank[id_col + 'x'].iloc[i]]

            if i_wins < 0:  # i+1 wins, swap rank_col vals
                temp = rank[rank_col].iloc[i]
                rank[rank_col].iloc[i] = rank[rank_col].iloc[i + 1]
                rank[rank_col].iloc[i + 1] = temp
            elif i_wins == 0:  # tie...set rank as mean of two
                new_rank = (rank[rank_col].iloc[i] + rank[rank_col].iloc[i + 1]) / 2
                rank[rank_col].iloc[i] = new_rank
                rank[rank_col].iloc[i + 1] = new_rank

    if A is not None and flag_tie == 1:  # sort again
        rank = rank.sort_values(by=rank_col, ascending=True).reset_index().drop(columns='index')

    return rank


def makeWeightVector(n_cols, groups, group_weights):
    weights = np.zeros((n_cols, 1))
    for i_group in range(len(groups)):
        if groups[i_group] in group_weights:
            weights[i_group] = group_weights[groups[i_group]]

    return weights
