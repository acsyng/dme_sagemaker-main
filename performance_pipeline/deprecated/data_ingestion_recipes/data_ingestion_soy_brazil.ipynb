{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a18a27-db3f-443d-8e3e-0cb50f52bd31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# necessary for DenodoConnection\n",
    "#%pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069b4310-95fb-408a-b3b3-ca1f50f54dab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT name from config_vars: uat\n",
      "DME_PROJECT name from config_vars: None\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# need to add dme_sagemaker to path to load in libraries\n",
    "sys.path.append(\"/root/dme_sagemaker/dme_sagemaker\")\n",
    "sys.path\n",
    "\n",
    "from libs.denodo.denodo_connection import DenodoConnection\n",
    "from libs.performance_lib import performance_sql_recipes\n",
    "from libs.performance_lib import performance_helper\n",
    "from libs.performance_lib import performance_validation_functions\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5b929d-3e16-49ab-853f-d4ac84b017f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket='us.com.syngenta.ap.nonprod' # Replace with your s3 bucket name\n",
    "\n",
    "ap_data_sector = 'SOY_BRAZIL_SUMMER'\n",
    "analysis_year = '2018'\n",
    "analysis_type = 'SingleExp'\n",
    "\n",
    "# STEP 0: define steps\n",
    "get_trial_pheno = 1\n",
    "get_pvs = 1\n",
    "get_plot_result = 1 \n",
    "get_rm = 1\n",
    "get_historical_stage = 1\n",
    "flag_to_write_to_s3 = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0772b82-381b-41a3-a3c5-793b5e74108e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name # not necessary\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def write_df_to_s3(df, \n",
    "                fname, \n",
    "                ap_data_sector, \n",
    "                analysis_type,\n",
    "                analysis_year,\n",
    "                s3_path='uat/dme/performance/compute_pred_adv_data_collected/data', s3_bucket='us.com.syngenta.ap.nonprod'):\n",
    "    \n",
    "    # write locally\n",
    "    local_fname = os.path.join('/root/dme_sagemaker/dme_sagemaker/performance_pipeline/data_ingestion_recipes',fname)\n",
    "    s3_fname = os.path.join(s3_path, ap_data_sector, analysis_type, analysis_year, fname)\n",
    "    \n",
    "    df.to_csv(\n",
    "        local_fname,\n",
    "        index=False,\n",
    "    )\n",
    "    \n",
    "    # upload to s3\n",
    "    s3.upload_file(\n",
    "        local_fname,\n",
    "        Bucket=bucket,\n",
    "        Key=s3_fname,\n",
    "        ExtraArgs={\n",
    "            'ServerSideEncryption':'aws:kms',\n",
    "            'SSEKMSKeyId':'arn:aws:kms:us-east-1:809800841141:key/353d6263-d454-444f-ac60-41afe025b445'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # delete local file\n",
    "    os.remove(local_fname)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e1c488-9ebb-4ffe-a827-dcd748d778f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 1: Get checks across trials and traits\n",
    "def compute_trial_checks(ap_data_sector,\n",
    "                         analysis_year,\n",
    "                         analysis_type,\n",
    "                         pipeline_runid='',\n",
    "                         write_output=0):\n",
    "    # Compute recipe outputs\n",
    "    with DenodoConnection() as dc:\n",
    "        data_sector_config = dc.get_data(\"\"\"\n",
    "                            SELECT \n",
    "                                ap_data_sector_name,\n",
    "                                spirit_crop_guid,\n",
    "                                entry_id_source\n",
    "                              FROM \"managed\".\"rv_ap_data_sector_config\"\n",
    "                            WHERE \"ap_data_sector_name\" = {0}\"\"\".format(\"'\" + ap_data_sector + \"'\"))\n",
    "\n",
    "    checks_df = performance_sql_recipes.merge_trial_check_entries(ap_data_sector,\n",
    "                                          analysis_year,\n",
    "                                          analysis_type,\n",
    "                                          data_sector_config[\"spirit_crop_guid\"].iloc[0],\n",
    "                                          data_sector_config[\"entry_id_source\"].iloc[0])\n",
    "\n",
    "    \n",
    "    checks_df[['cpifl', 'cperf', 'cagrf', 'cmatf', 'cregf', 'crtnf']] = checks_df[\n",
    "        ['cpifl', 'cperf', 'cagrf', 'cmatf', 'cregf', 'crtnf']].apply(\n",
    "        lambda x: ((x / checks_df['result_count']) > 0.25).astype(int))\n",
    "\n",
    "    checks_df = checks_df.drop(\n",
    "        columns=['result_count', 'fp_ltb', 'mp_ltb', 'fp_het_pool', 'mp_het_pool', 'untested_entry_display'])\n",
    "\n",
    "    if write_output == 1:\n",
    "        # check to see if file path exists\n",
    "        out_dir = '/opt/ml/processing/data/trial_checks/{}/{}/{}'.format(\n",
    "            ap_data_sector, analysis_type, analysis_year)\n",
    "        out_fname = 'trial_checks.parquet'\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        checks_df.to_parquet(os.path.join(out_dir, out_fname))\n",
    "        \n",
    "    return checks_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082bccaf-cc77-41fa-9118-2396d0de62f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 2: get and process data from trial pheno\n",
    "# process includes getting phenotypic observations relative to checks at the trial level, which is why we need to get trial-checks first\n",
    "def load_and_process_trial_pheno(ap_data_sector, analysis_type, analysis_year, df_cpifl_grouped):\n",
    "    # meta columns to output\n",
    "    common_melt_cols = ['ap_data_sector', 'analysis_year', 'source_id', 'trial_id', 'entry_identifier',\n",
    "                        'market_seg', 'decision_group_rm', 'et_value',\n",
    "                        'material_type', 'trait']\n",
    "\n",
    "    # make a list with entry_id instead of entry_identifier\n",
    "    common_melt_cols_entry_id = common_melt_cols.copy()\n",
    "    common_melt_cols_entry_id.remove('entry_identifier')\n",
    "    common_melt_cols_entry_id.append('entry_id')\n",
    "\n",
    "    # run code to get trial_pheno_data\n",
    "    df_trial_pheno = \\\n",
    "        performance_sql_recipes.get_trial_pheno_data_reduced_columns(ap_data_sector, int(analysis_year))\n",
    "\n",
    "    # remove columns like et_value, plot_barcode, etc.\n",
    "    # this could be done in the sql query, which would be faster and less memory intensive.\n",
    "    df_trial_pheno = df_trial_pheno[[\n",
    "        'ap_data_sector',\n",
    "        'analysis_year',\n",
    "        'trial_id',\n",
    "        'entry_id',\n",
    "        'source_id',\n",
    "        'market_segment',\n",
    "        'trait',\n",
    "        'maturity_group',\n",
    "        'dme_chkfl',\n",
    "        'irrigation',\n",
    "        'et_value',\n",
    "        'result_numeric_value'\n",
    "    ]]\n",
    "\n",
    "    # merge in check flags per trait\n",
    "    df_trial_pheno = df_trial_pheno.merge(\n",
    "        df_cpifl_grouped,\n",
    "        on=['ap_data_sector', 'analysis_year', 'source_id', 'entry_id'],\n",
    "        how='left'\n",
    "    ).rename(columns={'market_segment': 'market_seg'})\n",
    "\n",
    "    df_trial_pheno = performance_helper.get_chkfl(df_trial_pheno)\n",
    "\n",
    "    df_trial_pheno[['cpifl', 'chkfl']] = df_trial_pheno[['cpifl', 'chkfl']].fillna(value=0)\n",
    "    df_trial_pheno['material_type'] = df_trial_pheno['material_type'].fillna(value='entry')\n",
    "\n",
    "    # get trait values relative to check and trial mean\n",
    "    # if check mean does not exist, use trial mean\n",
    "    trial_group_cols = ['ap_data_sector','analysis_year','trial_id','trait']\n",
    "    score_col = 'result_numeric_value'\n",
    "    keep_cols = trial_group_cols.copy()\n",
    "    keep_cols.append(score_col)\n",
    "\n",
    "    # check mean for each trait\n",
    "    df_trial_pheno_checks_mean = df_trial_pheno[df_trial_pheno['chkfl']==1][keep_cols].groupby(\n",
    "        by=trial_group_cols\n",
    "    ).mean().reset_index() # chkfl is per trait\n",
    "    df_trial_pheno_checks_mean = df_trial_pheno_checks_mean.rename(\n",
    "        columns={'result_numeric_value': 'trial_mean_value'}\n",
    "    )\n",
    "\n",
    "    # trial mean for each trait\n",
    "    df_trial_pheno_mean = df_trial_pheno[keep_cols].groupby(by=trial_group_cols).mean().reset_index()\n",
    "    df_trial_pheno_mean = df_trial_pheno_mean.rename(columns={'result_numeric_value': 'trial_mean_value'})\n",
    "\n",
    "    # choose check mean, then trial mean if check mean does not exist\n",
    "    # to do this, stack two dataframes, groupby and choose the first value\n",
    "    df_trial_pheno_mean = pd.concat((df_trial_pheno_checks_mean, df_trial_pheno_mean), axis=0)\n",
    "    df_trial_pheno_mean = df_trial_pheno_mean.groupby(by=trial_group_cols).first().reset_index()\n",
    "\n",
    "    # merge in check or trial mean, then subtract and drop trial mean\n",
    "    df_trial_pheno = df_trial_pheno.merge(\n",
    "        df_trial_pheno_mean, on=['ap_data_sector', 'analysis_year', 'trial_id', 'trait']            \n",
    "    )\n",
    "\n",
    "    df_trial_pheno['result_diff'] = df_trial_pheno['result_numeric_value'] - df_trial_pheno['trial_mean_value']\n",
    "    df_trial_pheno = df_trial_pheno.drop(\n",
    "        columns=['trial_mean_value']\n",
    "    ).rename(\n",
    "        columns={'maturity_group':'decision_group_rm'}\n",
    "    )\n",
    "\n",
    "    # there are nan's in the stage, decision_group_rm, technology cols\n",
    "    # fill, do group by, replace with nan's\n",
    "    df_trial_pheno['decision_group_rm'] = df_trial_pheno['decision_group_rm'].fillna(value=-123)\n",
    "\n",
    "    # group by with aggregations listed below\n",
    "    agg_dict = {'result_numeric_value': 'mean',\n",
    "                'result_diff': 'mean',\n",
    "                'cpifl': 'max',\n",
    "                'chkfl': 'max'\n",
    "                }\n",
    "\n",
    "    df_trial_pheno_grouped = df_trial_pheno.groupby(\n",
    "        by=common_melt_cols_entry_id\n",
    "    ).agg(agg_dict).reset_index()\n",
    "\n",
    "    # drop df_trial_pheno data as it is no longer useful, free up memory\n",
    "    df_trial_pheno = pd.DataFrame()\n",
    "\n",
    "    # with multiple aggregations, fix columns. Don't append agg type for columns with a single aggregation\n",
    "    new_cols = []\n",
    "    for col in df_trial_pheno_grouped.columns:\n",
    "        if isinstance(col, list):\n",
    "            if col[0] in agg_dict and isinstance(agg_dict[col[0]], list) and len(agg_dict[col[0]]) > 1:\n",
    "                new_cols.append(col[0] + '_' + col[1])\n",
    "            else:\n",
    "                new_cols.append(col[0])\n",
    "        else:\n",
    "            new_cols.append(col)\n",
    "    df_trial_pheno_grouped.columns = new_cols\n",
    "\n",
    "    # rename result_diff_mean and result_numeric_value_mean\n",
    "    df_trial_pheno_grouped = df_trial_pheno_grouped.rename(columns={'result_diff': 'result_diff',\n",
    "                                                                    'result_numeric_value': 'result'})\n",
    "\n",
    "    # replace temp nan-vals with nan's\n",
    "    for col, nan_val in zip(['decision_group_rm'], [-123]):\n",
    "        df_trial_pheno_grouped[col][df_trial_pheno_grouped[col] == nan_val] = np.nan\n",
    "\n",
    "    # rename entry_id to entry_identifier to keep consistent with other tables\n",
    "    df_trial_pheno_grouped = df_trial_pheno_grouped.rename(columns={'entry_id': 'entry_identifier'})\n",
    "\n",
    "    # put in dummy market segment column...may need to change this later\n",
    "    df_trial_pheno_grouped['market_seg'] = 'all'\n",
    "\n",
    "    # melt df_trial_pheno_grouped into a tall format\n",
    "    df_trial_pheno_melt = pd.melt(\n",
    "        df_trial_pheno_grouped,\n",
    "        id_vars=common_melt_cols,\n",
    "        var_name='var',\n",
    "        value_name='value',\n",
    "        value_vars=['result','result_diff','chkfl']\n",
    "    )\n",
    "\n",
    "    # move trait into var name, then drop trait\n",
    "    df_trial_pheno_melt['var'] = df_trial_pheno_melt['var'] + '_' + df_trial_pheno_melt['trait']\n",
    "    df_trial_pheno_melt = df_trial_pheno_melt.drop(columns='trait')\n",
    "\n",
    "    # get cpifl for each entry, not trait-specific\n",
    "    common_melt_cols_no_trait = common_melt_cols.copy()\n",
    "    common_melt_cols_no_trait.remove('trait')\n",
    "    common_melt_cols_no_trait_cpifl = common_melt_cols_no_trait.copy()\n",
    "    common_melt_cols_no_trait_cpifl.append('cpifl')\n",
    "\n",
    "    df_trial_pheno_cpifl_melt = pd.melt(\n",
    "        df_trial_pheno_grouped[common_melt_cols_no_trait_cpifl].groupby(\n",
    "            by=common_melt_cols_no_trait\n",
    "        ).max().reset_index(),\n",
    "        id_vars=common_melt_cols_no_trait,\n",
    "        var_name='var',\n",
    "        value_name='value',\n",
    "        value_vars=['cpifl']\n",
    "    )\n",
    "\n",
    "    df_trial_pheno_melt = pd.concat((df_trial_pheno_melt, df_trial_pheno_cpifl_melt), axis=0)\n",
    "\n",
    "    return df_trial_pheno_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80a43a9-521f-4372-be0d-561512a6bcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 3: load geno prediction data\n",
    "def load_and_process_pvs_data(ap_data_sector, \n",
    "                              analysis_type,\n",
    "                              analysis_year,\n",
    "                              df_cpifl_grouped):\n",
    "    # analysis year is a str\n",
    "    # define common output columns\n",
    "    common_melt_cols = ['ap_data_sector','analysis_year','entry_identifier','source_id',\n",
    "                   'market_seg','stage','decision_group_rm','technology',\n",
    "                  'material_type','trait']\n",
    "\n",
    "    # load pvs_data directly, it's small for Soy. May need to iterate through for Corn?\n",
    "    df_pvs_no_bebid = performance_sql_recipes.merge_pvs_input(\n",
    "        ap_data_sector=ap_data_sector,\n",
    "        analysis_year=analysis_year,\n",
    "        analysis_type=analysis_type)\n",
    "    \n",
    "    # get mapper between line codes (pvs data) and be bids\n",
    "    df_material_mapper = performance_sql_recipes.get_material_mapper()  # default crop guid is soy's\n",
    "\n",
    "    # drop market seg from cpifl table if it is here. There are discrepancies between cpifl and pvs for market seg\n",
    "    if \"market_seg\" in df_cpifl_grouped.columns:\n",
    "        df_cpifl_grouped = df_cpifl_grouped.drop(columns='market_seg')\n",
    "\n",
    "    # for Soy, pvs data may not use be_bid...mapper maps between linecode, highname, abbr_code and be_bid\n",
    "    df_mapper_list = []\n",
    "    for map_col in ['abbr_code', 'highname', 'line_code', 'be_bid']:\n",
    "        if map_col == 'be_bid':\n",
    "            df_map = df_material_mapper[['be_bid']].dropna().drop_duplicates()\n",
    "            df_map['entry_identifier'] = df_map['be_bid']\n",
    "        else:\n",
    "            df_map = df_material_mapper[['be_bid', map_col]].dropna().drop_duplicates()\n",
    "            df_map = df_map.rename(columns={map_col: 'entry_identifier'})\n",
    "        df_mapper_list.append(df_map)\n",
    "\n",
    "    df_mapper = pd.concat(df_mapper_list, axis=0).groupby(by=['entry_identifier']).first().reset_index()\n",
    "\n",
    "    # get be bid from mapper, drop extra identifier column\n",
    "    df_pvs_data = df_pvs_no_bebid.merge(df_mapper, how='inner',on=['entry_identifier'])\n",
    "    df_pvs_data = df_pvs_data.drop(columns=['entry_identifier']).rename(columns={'be_bid': 'entry_identifier'}).drop_duplicates()\n",
    "\n",
    "    # merge check information into pvs_data\n",
    "    df_pvs_data_cpifl = df_pvs_data.merge(\n",
    "        df_cpifl_grouped, \n",
    "        how='left',\n",
    "        left_on=['ap_data_sector', 'analysis_type', 'analysis_year','entry_identifier', 'source_id', 'material_type'],\n",
    "        right_on=['ap_data_sector', 'analysis_type', 'analysis_year','entry_id', 'source_id', 'material_type']\n",
    "     )\n",
    "\n",
    "    # make chkfl column, which compresses the cperf,cagrf etc. columns into one\n",
    "    df_pvs_data_cpifl = performance_helper.get_chkfl(df_pvs_data_cpifl)\n",
    "    df_pvs_data_cpifl = df_pvs_data_cpifl.drop(columns=['dme_chkfl', 'dme_reg_x', 'dme_reg_y', 'dme_rm_est', 'entry_id']) \n",
    "\n",
    "    # stack trial_pheno and pvs_data\n",
    "    # drop some columns that are not common between pvs and trial_pheno, make columns that aren't common\n",
    "    df_pvs_data_pre_melt = df_pvs_data_cpifl.drop(\n",
    "        columns=['analysis_type'])\n",
    "    df_pvs_data_pre_melt['irrigation'] = 'NA'\n",
    "\n",
    "    # pivot to tall format (use melt function), then stack data.\n",
    "    # do marker data after this as we can take an average over all of these numeric traits\n",
    "    # can't take average over text traits\n",
    "    df_pvs_data_melt = pd.melt(\n",
    "        df_pvs_data_pre_melt,\n",
    "        id_vars=common_melt_cols,\n",
    "        var_name='var',\n",
    "        value_vars=['count', 'prediction', 'stderr']\n",
    "    )\n",
    "    \n",
    "    if df_pvs_data_melt.shape[0] > 0:\n",
    "        # move trait into var name, then drop trait\n",
    "        df_pvs_data_melt['var'] = df_pvs_data_melt['var'] + '_' + df_pvs_data_melt['trait']\n",
    "        df_pvs_data_melt = df_pvs_data_melt.drop(columns='trait')\n",
    "\n",
    "        # stage comes in as strings and integers. Convert to floats because some regions have decimal point stages\n",
    "        df_pvs_data_melt['stage'] = df_pvs_data_melt['stage'].astype(float)\n",
    "        \n",
    "    return df_pvs_data_melt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "761b59d5-9351-470b-8bac-a8fbd343b85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 4: Get text/marker traits from the plot\n",
    "def load_text_traits_from_plot_data(ap_data_sector, analysis_year):\n",
    "    df_plot_result = performance_sql_recipes.get_plot_result_data(\n",
    "        analysis_year=int(analysis_year),\n",
    "        crop_name='soybean'\n",
    "    )\n",
    "\n",
    "    if df_plot_result.shape[0] > 0:\n",
    "        # convert material id to bebid, get experiment id for df_plot_result\n",
    "        df_plot_result = df_plot_result[df_plot_result['alpha_value'].notna()]\n",
    "        df_plot_result = df_plot_result[['trait', 'year', 'entry_id', 'alpha_value']]\n",
    "        df_plot_result = df_plot_result.groupby(by=['trait', 'year', 'entry_id']).first().reset_index().rename(\n",
    "            columns={'year': 'analysis_year','entry_id': 'entry_identifier'}\n",
    "        )\n",
    "\n",
    "        df_plot_result = df_plot_result.rename(columns={'trait': 'var', 'alpha_value': 'alpha_value'})\n",
    "        df_plot_result['var'] = df_plot_result['var'].str.lower()\n",
    "\n",
    "    return df_plot_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da09226c-c180-4bfc-b490-085063558258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 5: get RM data across multiple tables\n",
    "def load_rm_data_across_datasets(ap_data_sector, analysis_year):\n",
    "    # get RM data for hybrids across 2 tables (rv_variety_trait_data, rv_material_trait_data)\n",
    "    # For corn, get RM for parents by averaging over offspring RM\n",
    "    # need rv_variety_entry_data to map material guids to be bids\n",
    "    # run a number of SQL recipes to get initial datasets\n",
    "    \n",
    "    # get mapper between line codes (pvs data) and be bids\n",
    "    df_material_mapper = performance_sql_recipes.get_material_mapper()  # default crop guid is soy's\n",
    "\n",
    "    # get hybrid rm data from 3 tables, stack on top of each other\n",
    "    df_material_trait_data = performance_sql_recipes.get_material_trait_data(\n",
    "        ap_data_sector=ap_data_sector,\n",
    "        analysis_year=analysis_year\n",
    "    )\n",
    "    \n",
    "    df_hybrid_rm1 = performance_helper.get_hybrid_rm(\n",
    "        df_material_trait_data,\n",
    "        DKU_DST_ap_data_sector=ap_data_sector,\n",
    "        DKU_DST_analysis_year=analysis_year\n",
    "    )\n",
    "    \n",
    "    df_hybrid_rm1 = df_hybrid_rm1[['analysis_year', 'ap_data_sector_name', 'entry_id', 'number_value']].rename(\n",
    "        columns={'ap_data_sector_name': 'ap_data_sector', 'number_value': 'rm_estimate'}\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    # get rm from variety entry and variety trait tables, merge together to get correct identifier\n",
    "    df_variety_entry_data = performance_sql_recipes.get_variety_entry_data(\n",
    "        ap_data_sector=ap_data_sector,\n",
    "        analysis_year=analysis_year\n",
    "    )\n",
    "    \n",
    "    df_variety_trait_data = performance_sql_recipes.get_variety_trait_data(ap_data_sector=ap_data_sector)\n",
    "    \n",
    "    df_variety_entry_trait = df_variety_entry_data.merge(\n",
    "        df_variety_trait_data,\n",
    "        on=['genetic_affiliation_guid', 'crop_guid'],\n",
    "        how='inner').drop_duplicates()\n",
    "\n",
    "    df_hybrid_rm2 = performance_helper.get_hybrid_rm(\n",
    "        df_variety_entry_trait,\n",
    "        DKU_DST_ap_data_sector=ap_data_sector,\n",
    "        DKU_DST_analysis_year=analysis_year\n",
    "    )\n",
    "\n",
    "    df_hybrid_rm2 = df_hybrid_rm2[['analysis_year', 'ap_data_sector_name', 'entry_id', 'number_value']].rename(\n",
    "        columns={\n",
    "            'ap_data_sector_name': 'ap_data_sector', 'number_value': 'rm_estimate'}\n",
    "    ).drop_duplicates()\n",
    "\n",
    "   \n",
    "    ############ TO BE IMPLEMENTED\n",
    "    # get rm from postgres table\n",
    "    \n",
    "    # concat RMs across source tables\n",
    "    df_hybrid_rm = pd.concat((df_hybrid_rm1, df_hybrid_rm2), axis=0).drop_duplicates()\n",
    "\n",
    "    if df_hybrid_rm.shape[0] > 0:\n",
    "        df_hybrid_rm = df_hybrid_rm.groupby(\n",
    "            by=['ap_data_sector', 'analysis_year', 'entry_id']).mean().reset_index()\n",
    "\n",
    "    # merge RM data using a small version of df_merged because RM is not trait specific.\n",
    "    if df_hybrid_rm.shape[0] > 0:\n",
    "        df_rm_data = df_hybrid_rm.rename(columns={'entry_id': 'entry_identifier'})\n",
    "\n",
    "        # put df_rm_data in a tall format so that its format matches all other files\n",
    "        df_rm_melt = pd.melt(\n",
    "            df_rm_data,\n",
    "            id_vars=['ap_data_sector', 'analysis_year', 'entry_identifier'],\n",
    "            var_name='var',\n",
    "            value_vars=['rm_estimate']\n",
    "        ).dropna(subset=['value'])\n",
    "    else: # empty dataframe\n",
    "        df_rm_melt = pd.DataFrame(columns=['ap_data_sector','analysis_year','entry_identifier','var','rm_estimate'])\n",
    "\n",
    "    return df_rm_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8639d61a-f918-4676-b335-81d30d7dc286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 6: infer historical advancement decisions by looking at the stage materials were planted in each year\n",
    "def stack_bebids(df_in):\n",
    "    df_list = []\n",
    "    for col in ['be_bid', 'fp_be_bid', 'mp_be_bid']:\n",
    "        df_temp = df_in[['ap_data_sector_id', 'ap_data_sector_name', 'year', col, 'stage_lid']].rename(\n",
    "            columns={col: 'be_bid'})\n",
    "        df_temp = df_temp[(df_temp['be_bid'].notna()) & (df_temp['be_bid'] != '')]\n",
    "        if col == 'be_bid':\n",
    "            df_temp['material_type'] = 'entry'\n",
    "        else:\n",
    "            df_temp['material_type'] = 'parent'\n",
    "        df_list.append(df_temp)\n",
    "\n",
    "    df_stack = pd.concat(df_list, axis=0)\n",
    "    return df_stack\n",
    "\n",
    "\n",
    "def get_bebid_advancement_decisions(ap_data_sector, analysis_year, write_outputs=0):\n",
    "    # get be_bid, and parental be_bids, plus stage tested from denodo\n",
    "    df = performance_sql_recipes.get_material_by_trialstage_year_one_sector(ap_data_sector=ap_data_sector,min_year=int(analysis_year)-2, max_year=int(analysis_year)+3)\n",
    "\n",
    "    # stack be_bid, fp_be_bid, mp_be_bid, create material_type column\n",
    "    df_stack = stack_bebids(df)\n",
    "    df_stack = df_stack.drop_duplicates()\n",
    "\n",
    "    yrs = pd.unique(df_stack['year'])\n",
    "    yrs = np.sort(yrs)\n",
    "\n",
    "    # pivot dataframe by year\n",
    "    df_stack_piv = df_stack.pivot_table(values=['stage_lid'],\n",
    "                                        index=['ap_data_sector_id', 'ap_data_sector_name', 'be_bid',\n",
    "                                               'material_type'],\n",
    "                                        columns='year',\n",
    "                                        aggfunc='max').reset_index()\n",
    "    df_stack_piv.columns = ['_'.join((col_tuple[0], str(col_tuple[1]))).replace('value', '').strip('_') for\n",
    "                            col_tuple in df_stack_piv.columns]\n",
    "    # rename columns\n",
    "    col_rename = {}\n",
    "    stage_cols = []\n",
    "    for yr in yrs:\n",
    "        col_rename['stage_lid_' + str(yr)] = 'stage_' + str(yr)\n",
    "        stage_cols.append('stage_' + str(yr))\n",
    "    df_stack_piv = df_stack_piv.rename(columns=col_rename)\n",
    "    # get stage max column\n",
    "    df_stack_piv['stage_max'] = df_stack_piv[stage_cols].max(axis=1)\n",
    "\n",
    "    # clean year-stage info\n",
    "    for yr in yrs[1:]:\n",
    "        # if current year is null\n",
    "        # ->if year prior is not null\n",
    "        curr_stage_col = 'stage_' + str(yr)\n",
    "        prev_stage_col = 'stage_' + str(yr - 1)\n",
    "        future_stage_cols = ['stage_' + str(yr_next) for yr_next in\n",
    "                             range(yr + 1, np.minimum(yr + 5, yrs[-1]) + 1)]\n",
    "\n",
    "        # ->if all of future years are null = 13\n",
    "        # else = stage from prev year\n",
    "        adj_mask = (df_stack_piv[curr_stage_col].isna()) & (df_stack_piv[prev_stage_col].notna())\n",
    "        future_mask = np.all(df_stack_piv[future_stage_cols].isna(), axis=1)\n",
    "\n",
    "        df_stack_piv[curr_stage_col][(adj_mask) & (future_mask)] = 13\n",
    "        df_stack_piv[curr_stage_col][(adj_mask) & (future_mask == False)] = df_stack_piv[prev_stage_col][\n",
    "            (adj_mask) & (future_mask == False)]\n",
    "\n",
    "    # make stage achieved columns, fill with 0 to start\n",
    "    stages = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    for stage in stages:\n",
    "        if stage < 7:\n",
    "            stage_name = 'stage_' + str(stage)\n",
    "        else:\n",
    "            stage_name = 'stage_chk'\n",
    "        df_stack_piv[stage_name] = np.any(df_stack_piv[stage_cols].values == stage, axis=1).astype(int)\n",
    "\n",
    "    # write recipe outputs, set index as false so when this gets loaded there aren't two indices\n",
    "    if write_outputs==1:\n",
    "        out_dir = '/opt/ml/processing/data/bebid_advancement_decisions/{}'.format(ap_data_sector)\n",
    "        out_fname = 'bebid_advancement_decisions.csv'\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        df_stack_piv.to_csv(os.path.join(out_dir, out_fname), index=False)\n",
    "\n",
    "    return df_stack_piv\n",
    "\n",
    "\n",
    "def load_bebid_stage(df_stage_piv, ap_data_sector, analysis_year):\n",
    "    # merge in stage information from past and future years\n",
    "    # setup variables for extracting appropriate stage\n",
    "    stage_str_list = ['prev_stage', 'current_stage', 'next_stage', 'third_stage']\n",
    "    yr_offset_list = [-1, 0, 1, 2]\n",
    "\n",
    "    df_stage_list = []\n",
    "\n",
    "    # rename columns and drop materials from other data sectors\n",
    "    df_stage_piv = df_stage_piv.rename(\n",
    "        columns={'ap_data_sector_name': 'ap_data_sector',\n",
    "                 'material_type': 'material_type_simple',\n",
    "                 'be_bid': 'entry_identifier'}\n",
    "    )\n",
    "\n",
    "    # rename stage columns for the previous, current, next and next-next year\n",
    "    stages_to_get = []\n",
    "    for i_yr in range(len(stage_str_list)):\n",
    "        year_to_get = int(analysis_year) + yr_offset_list[i_yr]\n",
    "        stage_str = 'stage_' + str(year_to_get)\n",
    "        if stage_str in df_stage_piv.columns:\n",
    "            df_stage_piv = df_stage_piv.rename(columns={stage_str: stage_str_list[i_yr]})\n",
    "            stages_to_get.append(stage_str_list[i_yr])\n",
    "\n",
    "    # only keep meta columns and columns corresponding to years around analysis_year\n",
    "    cols_to_keep = ['ap_data_sector', 'material_type_simple', 'entry_identifier']\n",
    "    if len(stages_to_get) > 0:\n",
    "        cols_to_keep.extend(stages_to_get)\n",
    "    df_stage_piv = df_stage_piv[cols_to_keep]\n",
    "\n",
    "    # drop materials with a 13 in current_stage\n",
    "    df_stage_piv = df_stage_piv[df_stage_piv['current_stage'] < 13]\n",
    "\n",
    "    # create analysis year columns\n",
    "    df_stage_piv['analysis_year'] = int(analysis_year)\n",
    "    \n",
    "    # make empty stage columns in df_merged if they don't exist\n",
    "    # for the current year, we may not see next and third year data\n",
    "    for stage_str in stage_str_list:\n",
    "        if stage_str not in df_stage_piv.columns:\n",
    "            df_stage_piv[stage_str] = 13\n",
    "\n",
    "    # get advancement decisions by comparing stage information across years\n",
    "    df_stage_piv['was_adv'] = performance_validation_functions.checkAdvancement(\n",
    "        df_stage_piv,\n",
    "        current_stage='current_stage',\n",
    "        next_stage='next_stage'\n",
    "    )\n",
    "\n",
    "    df_stage_piv['was_adv_next'] = (df_stage_piv['was_adv']) & performance_validation_functions.checkAdvancement(\n",
    "        df_stage_piv,\n",
    "        current_stage='next_stage',\n",
    "        next_stage='third_stage'\n",
    "    )\n",
    "    \n",
    "    # make sure these variables have type bool\n",
    "    df_stage_piv['was_adv'] = df_stage_piv['was_adv'].astype(bool)\n",
    "    df_stage_piv['was_adv_next'] = df_stage_piv['was_adv_next'].astype(bool)\n",
    "            \n",
    "    # melt data\n",
    "    df_adv_dec_melt = pd.melt(\n",
    "        df_stage_piv,\n",
    "        id_vars=['ap_data_sector','material_type_simple','entry_identifier'],\n",
    "        var_name='var',\n",
    "        value_vars=['prev_stage', 'current_stage', 'next_stage', 'third_stage', 'was_adv','was_adv_next']\n",
    "    ).dropna(subset=['value'])\n",
    "        \n",
    "    return df_adv_dec_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c502acc-9404-4cbf-9a1f-c82624434f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting checks\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Get checks across trials and traits. Data from trial pheno and geno prediction currently requires trial checks.\n",
    "# no other step requires this information\n",
    "print(\"getting checks\")\n",
    "df_checks = compute_trial_checks(ap_data_sector=ap_data_sector, analysis_year=analysis_year, analysis_type=analysis_type)\n",
    "\n",
    "# aggregate check information up to the material per source id level.\n",
    "cpifl_group_cols = [\n",
    "    'ap_data_sector', 'analysis_type', 'analysis_year',\n",
    "    'source_id','entry_id', 'material_type'\n",
    "]\n",
    "\n",
    "cpifl_info_cols = ['cpifl', 'cperf', 'cagrf', 'cmatf', 'cregf', 'crtnf']\n",
    "cpifl_keep_cols = cpifl_group_cols.copy()\n",
    "cpifl_keep_cols.extend(cpifl_info_cols)\n",
    "df_cpifl_grouped = df_checks[cpifl_keep_cols].groupby(by=cpifl_group_cols).max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57edd21b-d265-4409-9338-abbcbe4a53ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting trial pheno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dme_sagemaker/dme_sagemaker/libs/performance_lib/performance_helper.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_out['chkfl'][df_out['dme_chkfl']==check_name] = df_out[check_name][df_out['dme_chkfl']==check_name]\n",
      "/root/dme_sagemaker/dme_sagemaker/libs/performance_lib/performance_helper.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_out['chkfl'][df_out['dme_chkfl']==check_name] = df_out[check_name][df_out['dme_chkfl']==check_name]\n",
      "/root/dme_sagemaker/dme_sagemaker/libs/performance_lib/performance_helper.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_out['chkfl'][df_out['dme_chkfl']==check_name] = df_out[check_name][df_out['dme_chkfl']==check_name]\n",
      "/root/dme_sagemaker/dme_sagemaker/libs/performance_lib/performance_helper.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_out['chkfl'][df_out['dme_chkfl']==check_name] = df_out[check_name][df_out['dme_chkfl']==check_name]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: get and process data from trial pheno\n",
    "if get_trial_pheno == 1 or 1==1:\n",
    "    print(\"getting trial pheno\")\n",
    "    df_trial_pheno_melt = load_and_process_trial_pheno(\n",
    "        ap_data_sector=ap_data_sector,\n",
    "        analysis_type=analysis_type,\n",
    "        analysis_year=analysis_year,\n",
    "        df_cpifl_grouped=df_cpifl_grouped\n",
    "    )\n",
    "    if flag_to_write_to_s3 == 1:\n",
    "        # save files to s3\n",
    "        write_df_to_s3(\n",
    "            df_trial_pheno_melt,\n",
    "            fname='new_trial_pheno_tall.csv', \n",
    "            ap_data_sector=ap_data_sector,\n",
    "            analysis_type=analysis_type,\n",
    "            analysis_year=analysis_year\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ab5144-b5fe-4435-8e4f-5f4534606998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting pvs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting plot result\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: load geno prediction data\n",
    "if get_pvs == 1:\n",
    "    print(\"getting pvs\")\n",
    "    df_pvs_data_melt = load_and_process_pvs_data(\n",
    "        ap_data_sector=ap_data_sector, \n",
    "        analysis_type=analysis_type,\n",
    "        analysis_year=analysis_year,\n",
    "        df_cpifl_grouped=df_cpifl_grouped\n",
    "    )\n",
    "    if flag_to_write_to_s3 == 1:\n",
    "        # save files to s3\n",
    "        write_df_to_s3(\n",
    "            df_pvs_data_melt,\n",
    "            fname='new_pvs_data_tall.csv', \n",
    "            ap_data_sector=ap_data_sector,\n",
    "            analysis_type=analysis_type,\n",
    "            analysis_year=analysis_year\n",
    "        )\n",
    "\n",
    "# STEP 4: get text/marker traits from plot data\n",
    "if get_plot_result == 1:\n",
    "    print(\"getting plot result\")\n",
    "    df_plot_result = load_text_traits_from_plot_data(ap_data_sector=ap_data_sector, analysis_year=analysis_year)\n",
    "    if flag_to_write_to_s3 == 1:\n",
    "        # save files to s3\n",
    "        write_df_to_s3(\n",
    "            df_plot_result,\n",
    "            fname='new_plot_result_tall.csv', \n",
    "            ap_data_sector=ap_data_sector,\n",
    "            analysis_type=analysis_type,\n",
    "            analysis_year=analysis_year\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3799c13d-9c31-46f3-8eb1-cb9e761828ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting RM\n"
     ]
    }
   ],
   "source": [
    "if get_rm  == 1:\n",
    "    print(\"getting RM\")\n",
    "    df_rm = load_rm_data_across_datasets(ap_data_sector=ap_data_sector, analysis_year=analysis_year)\n",
    "    \n",
    "    if flag_to_write_to_s3 == 1:\n",
    "        # save files to s3\n",
    "        write_df_to_s3(\n",
    "            df_rm,\n",
    "            fname='new_RM_tall.csv', \n",
    "            ap_data_sector=ap_data_sector,\n",
    "            analysis_type=analysis_type,\n",
    "            analysis_year=analysis_year\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfee3f3a-0dee-4a9d-979a-f3be974b581d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting historical stages\n",
      "starting denodo connection\n",
      "getting data\n",
      "received data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: load historical advancement decisions\n",
    "if get_historical_stage:\n",
    "    print(\"getting historical stages\")\n",
    "    df_stage_piv = get_bebid_advancement_decisions(ap_data_sector=ap_data_sector, analysis_year=analysis_year, write_outputs=0)\n",
    "    df_adv_dec_melt = load_bebid_stage(df_stage_piv=df_stage_piv, ap_data_sector=ap_data_sector, analysis_year=analysis_year)\n",
    "    \n",
    "    if flag_to_write_to_s3 == 1:\n",
    "        # save files to s3\n",
    "        write_df_to_s3(\n",
    "            df_adv_dec_melt,\n",
    "            fname='new_decisions_tall.csv', \n",
    "            ap_data_sector=ap_data_sector,\n",
    "            analysis_type=analysis_type,\n",
    "            analysis_year=analysis_year\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a9db367-7d93-481d-a6c0-8468a860c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Load decision groups if applicable. \n",
    "df_decision_groups = performance_sql_recipes.get_decision_groups(\n",
    "    ap_data_sector,\n",
    "    analysis_year\n",
    ")\n",
    "\n",
    "if flag_to_write_to_s3 == 1:\n",
    "        # save files to s3\n",
    "        write_df_to_s3(\n",
    "            df_decision_groups,\n",
    "            fname='new_decision_groups_tall.csv', \n",
    "            ap_data_sector=ap_data_sector,\n",
    "            analysis_type=analysis_type,\n",
    "            analysis_year=analysis_year\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
